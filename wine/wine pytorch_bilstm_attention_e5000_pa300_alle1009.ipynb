{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CyEli48EB0_C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686285835359,"user_tz":240,"elapsed":9654,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"5c4b6cf1-2b22-4a11-a459-a03189e0fcc1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f98605e5e70>"]},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt   \n","\n","import torch\n","import torch.nn as nn\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence\n","\n","import pickle\n","\n","\n","torch.manual_seed(1)"]},{"cell_type":"code","source":["rela_embedding=pd.read_csv('./data/KRAL_Embedding_Model_WINE_D032-rela.csv')\n","tail_embedding=pd.read_csv('./data/KRAL_Embedding_Model_WINE_D032-tail.csv')\n","x_test = pd.read_csv(\"./data/wine_x_test.csv\").drop('Unnamed: 0',axis=1)\n","polish_train_val=pd.read_csv(\"./data/wine_train_val.csv\").drop('Unnamed: 0',axis=1)\n","y_train_val=pd.read_csv(\"./data/y_wine_trainval.csv\").drop('Unnamed: 0',axis=1)\n","y_test=pd.read_csv(\"./data/y_test_wine.csv\").drop('Unnamed: 0',axis=1)\n"],"metadata":{"id":"ng_oD9z1nwsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sum(y_train_val['quality'])/len(y_train_val))\n","print(sum(y_test['quality'])/len(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxqGnPX7stHD","executionInfo":{"status":"ok","timestamp":1686253245733,"user_tz":240,"elapsed":17,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"4b498b0e-0d2b-491e-f104-5186e454c93f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.34959871589085073\n","0.3443223443223443\n"]}]},{"cell_type":"code","source":["filename='./data/wine_test_notnull.pickle'\n","with open(filename, 'rb') as f:\n","    # deserialize the list and load it from the file\n","    loaded_test_notnull_lst = pickle.load(f)\n","\n","test_notnull_len=[]\n","for i in range(len(loaded_test_notnull_lst)):\n","    test_notnull_len.append(len(loaded_test_notnull_lst[i]))\n","\n","test_integrate=[]\n","for i in range(len(loaded_test_notnull_lst)):\n","  test_integrate.append(torch.tensor(loaded_test_notnull_lst[i]))"],"metadata":{"id":"UOvsUUlYs0wb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainval_filename='./data/wine_train_val_notnull.pickle'\n","with open(trainval_filename, 'rb') as f:\n","    # deserialize the list and load it from the file\n","    loaded_trainval_notnull_lst = pickle.load(f)"],"metadata":{"id":"ZN6EWCQ7tK8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainval_len=[]\n","for i in range(len(loaded_trainval_notnull_lst)):\n","    trainval_len.append(len(loaded_trainval_notnull_lst[i]))"],"metadata":{"id":"rwPLLzzat0SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","test_notnull_count = Counter(test_notnull_len)\n","trainval_count=Counter(trainval_len)\n","print(test_notnull_count)\n","print(trainval_count)\n","\n","test_batch_avg=round(np.average(list(dict(test_notnull_count).keys())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUdXNf3ztcl3","executionInfo":{"status":"ok","timestamp":1686253246730,"user_tz":240,"elapsed":13,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"0c9f82fc-94da-4d1e-ff61-1859f0823f01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({9: 232, 10: 192, 8: 96, 7: 22, 11: 3, 6: 1})\n","Counter({9: 1231, 10: 1214, 8: 541, 7: 102, 11: 12, 6: 11, 12: 3, 5: 1})\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","# define a custom dataset class\n","class MyDataset(Dataset):\n","    def __init__(self, data, labels):\n","        # sort the data and labels based on the length of each item in data\n","        self.data, self.labels = zip(*sorted(zip(data, labels), key=lambda x: len(x[0]), reverse=True))\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        x = torch.tensor(self.data[idx])\n","        y = self.labels[idx]\n","        return x, y\n","\n","def collate_fn(data):\n","    input_data = [d[0] for d in data]\n","    y = [d[1] for d in data]\n","    input_data = pad_sequence(input_data, batch_first=True)\n","    input_data = torch.flip(input_data, dims=[1])\n","    return input_data, torch.tensor(y)\n","\n"],"metadata":{"id":"QT2wHxYNtdjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_batch_avg=round(np.average(list(dict(test_notnull_count).keys())))\n","\n","\n","test_dataset = MyDataset(test_integrate, y_test['quality'].values.tolist())\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=test_batch_avg, collate_fn=collate_fn)\n","\n","# iterate over batches of data and labels\n","for batch_data, batch_labels in test_dataloader:\n","    # print the batch data and labels\n","    print(batch_data.size())\n","    print(batch_labels.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0U5O2FTIuOqB","executionInfo":{"status":"ok","timestamp":1686253250660,"user_tz":240,"elapsed":230,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"ed580c2c-0967-4525-a921-01659e39807d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 11, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 10, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 9, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 8, 64])\n","torch.Size([8])\n","torch.Size([8, 7, 64])\n","torch.Size([8])\n","torch.Size([8, 7, 64])\n","torch.Size([8])\n","torch.Size([2, 7, 64])\n","torch.Size([2])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-10-405d3086c905>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(self.data[idx])\n"]}]},{"cell_type":"code","source":["with open('./data/train_data.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    train_data=pickle.load(f)\n","\n","with open('./data/val_data.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    val_data=pickle.load(f)\n","\n","with open('./data/train_y.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    train_y=pickle.load(f)\n","\n","with open('./data/val_y.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    val_y=pickle.load(f)"],"metadata":{"id":"vbQPyNwUN4F8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def batch_size_func(data_tensor):\n","  length_lst=[]\n","  for i in range(len(data_tensor)):\n","    length_lst.append(len(data_tensor[i]))\n","  len_counter=Counter(length_lst)\n","  batch_avg=round(np.average(list(dict(len_counter).keys())))\n","  return batch_avg\n"],"metadata":{"id":"QcCHtWGhzTTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_batch_size_avg=batch_size_func(train_data)\n","val_batch_size_avg=batch_size_func(val_data)\n","train_dataset = MyDataset(train_data, train_y)\n","val_dataset=MyDataset(val_data, val_y)\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size_avg, collate_fn=collate_fn)\n","val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size_avg, collate_fn=collate_fn)\n","\n","for batch_data, batch_labels in val_dataloader:\n","    print(batch_data.size())\n","    print(batch_labels.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JykYG4ewzjuS","executionInfo":{"status":"ok","timestamp":1686253275957,"user_tz":240,"elapsed":373,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"3f07a054-c03d-4bb2-fdd8-dab00c358bdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([9, 12, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 10, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 9, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 8, 64])\n","torch.Size([9])\n","torch.Size([9, 7, 64])\n","torch.Size([9])\n","torch.Size([9, 7, 64])\n","torch.Size([9])\n","torch.Size([9, 7, 64])\n","torch.Size([9])\n","torch.Size([8, 7, 64])\n","torch.Size([8])\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class bilstm_attention(nn.Module):\n","    def __init__(self, input_size, hidden_units,num_classes):\n","        super().__init__()\n","        self.input_size = input_size  # this is the number of features\n","        self.num_classes=num_classes\n","        self.hidden_units = hidden_units\n","        \n","        self.num_layers = 2\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_units,\n","            batch_first=True,\n","            num_layers=self.num_layers,\n","            bidirectional=True,\n","            dropout=0.1\n","        )\n","\n","        self.tanh1 = nn.Tanh()\n","        self.w = nn.Parameter(torch.Tensor(self.hidden_units * 2, 1))\n","        self.tanh2 = nn.Tanh()\n","        self.fc = nn.Linear(self.hidden_units * 2, self.num_classes)\n","        nn.init.uniform_(self.w, -0.1, 0.1)\n","\n","       \n","\n","    def forward(self, x):\n","        H, _ = self.lstm(x, None) # [batch_size, seq_len, hidden_size * 2]\n","        M = self.tanh1(H)  # [batch_size, seq_len, hidden_size * 2]\n","        # tensor operation\n","        alpha = F.softmax(torch.matmul(M, self.w), dim=1)# [batch_size, seq_len, 1]\n","        # When tensor elements are multiplied, tensor broadcasting occurs so that the dimensions of the tensor satisfy the condition\n","        out = H * alpha  # [batch_size, seq_len, hidden_size * 2]\n","        out = torch.sum(out, 1) # [batch_size,hidden_size * 2]  \n","        out = self.tanh2(out)\n","        out = self.fc(out)# [batch_size,num_classes]\n","        return out\n","        "],"metadata":{"id":"RqtfaAVQIyxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_data,batch_label=next(iter(train_dataloader))\n","model = bilstm_attention(input_size=batch_data.size()[2], hidden_units=512,num_classes=1)\n","re_out=model(batch_data)\n","print(batch_data.size())\n","print(re_out.size())\n","print(re_out)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSbgZqSBzyu6","executionInfo":{"status":"ok","timestamp":1686253592343,"user_tz":240,"elapsed":396,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"c9a7ca88-c2df-4e31-a158-8b07df2b7fe7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 12, 64])\n","torch.Size([8, 1])\n","tensor([[-0.0035],\n","        [-0.0036],\n","        [-0.0036],\n","        [-0.0020],\n","        [-0.0028],\n","        [-0.0021],\n","        [-0.0037],\n","        [-0.0035]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["def train_model(device,train_loader, val_loader, model, patience, n_epochs):\n","    \n","    # to track the training loss as the model trains\n","    train_losses = []\n","    # to track the validation loss as the model trains\n","    valid_losses = []\n","    # to track the average training loss per epoch as the model trains\n","    avg_train_losses = []\n","    # to track the average validation loss per epoch as the model trains\n","    avg_valid_losses = [] \n","    \n","    num_batches_train = len(train_loader)\n","    total_loss = 0\n","\n","    # initialize the early_stopping object\n","    early_stopping = EarlyStopping(patience=patience, verbose=True)\n","    \n","    for epoch in range(1, n_epochs + 1):\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train() # prep model for training\n","        for X, y in train_loader:\n","          \n","          X=X.to(device) \n","          y=y.to(device,dtype=torch.float)\n","          #y=y.float() \n","          output = model(X) \n","          output=output.squeeze()\n","          \n","          # clear the gradients of all optimized variables\n","          optimizer.zero_grad()\n","          \n","          # calculate the loss\n","          loss = criterion(output, y)\n","          # backward pass: compute gradient of the loss with respect to model parameters\n","          loss.backward()\n","          # perform a single optimization step (parameter update)\n","          optimizer.step()\n","          # record training loss\n","          train_losses.append(loss.item())\n","\n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval() # prep model for evaluation\n","        for data, target in val_loader:\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            data=data.to(device) \n","            target=target.to(device,dtype=torch.float)\n","            output = model(data)\n","            output=output.squeeze()\n","            #target=target.float()\n","            # calculate the loss\n","            loss = criterion(output, target)\n","            # record validation loss\n","            valid_losses.append(loss.item())\n","\n","        # print training/validation statistics \n","        # calculate average loss over an epoch\n","        train_loss = np.average(train_losses)\n","        valid_loss = np.average(valid_losses)\n","        avg_train_losses.append(train_loss)\n","        avg_valid_losses.append(valid_loss)\n","        \n","        epoch_len = len(str(n_epochs))\n","        \n","        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n","                     f'train_loss: {train_loss:.5f} ' +\n","                     f'valid_loss: {valid_loss:.5f}')\n","        \n","        print(print_msg)\n","        \n","        # clear lists to track next epoch\n","        train_losses = []\n","        valid_losses = []\n","        \n","        # early_stopping needs the validation loss to check if it has decresed, \n","        # and if it has, it will make a checkpoint of the current model\n","        early_stopping(valid_loss, model,n_epochs+1, optimizer,criterion)\n","        \n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","        \n","    # load the last checkpoint with the best model\n","    checkpoint = torch.load('./pytorch/model/epoch5000_pa300_all1009/save_checkpoint.pt')\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    with open('./pytorch/model/epoch5000_pa300_all1009/avg_train_losses.pickle', 'wb') as f:\n","      pickle.dump(avg_train_losses, f)\n","    with open('./pytorch/model/epoch5000_pa300_all1009/avg_valid_losses.pickle', 'wb') as f:\n","      pickle.dump(avg_valid_losses, f)\n","    return model, avg_train_losses, avg_valid_losses"],"metadata":{"id":"Cfy5_Bv7b8M6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZoFQKrf3yVJm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["patience 300, epoch 5000, optimzier SGD"],"metadata":{"id":"OUOw77b2h8GU"}},{"cell_type":"code","source":["from pytorchtools_save_epoch import EarlyStopping\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","model = bilstm_attention(input_size=batch_data.size()[2], hidden_units=512,num_classes=1).to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001,momentum=0.85)\n","patience=300\n","bilstm_atten_savemodel_patience_300,train_loss,valid_loss = train_model(device,train_dataloader, val_dataloader, model, patience, 5000)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cv9mltT20DSn","executionInfo":{"status":"ok","timestamp":1683187220061,"user_tz":240,"elapsed":1993558,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"c4ad14dc-0889-4da5-ca50-24351e908311"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[   1/5000] train_loss: 0.67337 valid_loss: 0.65943\n","Validation loss decreased (inf --> 0.659433).  Saving model ...\n","[   2/5000] train_loss: 0.65388 valid_loss: 0.65038\n","Validation loss decreased (0.659433 --> 0.650376).  Saving model ...\n","[   3/5000] train_loss: 0.64890 valid_loss: 0.64801\n","Validation loss decreased (0.650376 --> 0.648009).  Saving model ...\n","[   4/5000] train_loss: 0.64758 valid_loss: 0.64735\n","Validation loss decreased (0.648009 --> 0.647349).  Saving model ...\n","[   5/5000] train_loss: 0.64718 valid_loss: 0.64714\n","Validation loss decreased (0.647349 --> 0.647141).  Saving model ...\n","[   6/5000] train_loss: 0.64704 valid_loss: 0.64706\n","Validation loss decreased (0.647141 --> 0.647061).  Saving model ...\n","[   7/5000] train_loss: 0.64698 valid_loss: 0.64702\n","Validation loss decreased (0.647061 --> 0.647023).  Saving model ...\n","[   8/5000] train_loss: 0.64694 valid_loss: 0.64700\n","Validation loss decreased (0.647023 --> 0.646998).  Saving model ...\n","[   9/5000] train_loss: 0.64691 valid_loss: 0.64698\n","Validation loss decreased (0.646998 --> 0.646980).  Saving model ...\n","[  10/5000] train_loss: 0.64692 valid_loss: 0.64696\n","Validation loss decreased (0.646980 --> 0.646964).  Saving model ...\n","[  11/5000] train_loss: 0.64690 valid_loss: 0.64695\n","Validation loss decreased (0.646964 --> 0.646950).  Saving model ...\n","[  12/5000] train_loss: 0.64689 valid_loss: 0.64694\n","Validation loss decreased (0.646950 --> 0.646935).  Saving model ...\n","[  13/5000] train_loss: 0.64688 valid_loss: 0.64692\n","Validation loss decreased (0.646935 --> 0.646921).  Saving model ...\n","[  14/5000] train_loss: 0.64686 valid_loss: 0.64691\n","Validation loss decreased (0.646921 --> 0.646908).  Saving model ...\n","[  15/5000] train_loss: 0.64684 valid_loss: 0.64689\n","Validation loss decreased (0.646908 --> 0.646894).  Saving model ...\n","[  16/5000] train_loss: 0.64683 valid_loss: 0.64688\n","Validation loss decreased (0.646894 --> 0.646880).  Saving model ...\n","[  17/5000] train_loss: 0.64681 valid_loss: 0.64687\n","Validation loss decreased (0.646880 --> 0.646865).  Saving model ...\n","[  18/5000] train_loss: 0.64680 valid_loss: 0.64685\n","Validation loss decreased (0.646865 --> 0.646851).  Saving model ...\n","[  19/5000] train_loss: 0.64678 valid_loss: 0.64684\n","Validation loss decreased (0.646851 --> 0.646837).  Saving model ...\n","[  20/5000] train_loss: 0.64676 valid_loss: 0.64682\n","Validation loss decreased (0.646837 --> 0.646823).  Saving model ...\n","[  21/5000] train_loss: 0.64674 valid_loss: 0.64681\n","Validation loss decreased (0.646823 --> 0.646808).  Saving model ...\n","[  22/5000] train_loss: 0.64673 valid_loss: 0.64679\n","Validation loss decreased (0.646808 --> 0.646793).  Saving model ...\n","[  23/5000] train_loss: 0.64671 valid_loss: 0.64678\n","Validation loss decreased (0.646793 --> 0.646779).  Saving model ...\n","[  24/5000] train_loss: 0.64671 valid_loss: 0.64676\n","Validation loss decreased (0.646779 --> 0.646764).  Saving model ...\n","[  25/5000] train_loss: 0.64668 valid_loss: 0.64675\n","Validation loss decreased (0.646764 --> 0.646749).  Saving model ...\n","[  26/5000] train_loss: 0.64666 valid_loss: 0.64673\n","Validation loss decreased (0.646749 --> 0.646734).  Saving model ...\n","[  27/5000] train_loss: 0.64664 valid_loss: 0.64672\n","Validation loss decreased (0.646734 --> 0.646719).  Saving model ...\n","[  28/5000] train_loss: 0.64663 valid_loss: 0.64670\n","Validation loss decreased (0.646719 --> 0.646703).  Saving model ...\n","[  29/5000] train_loss: 0.64663 valid_loss: 0.64669\n","Validation loss decreased (0.646703 --> 0.646688).  Saving model ...\n","[  30/5000] train_loss: 0.64661 valid_loss: 0.64667\n","Validation loss decreased (0.646688 --> 0.646673).  Saving model ...\n","[  31/5000] train_loss: 0.64657 valid_loss: 0.64666\n","Validation loss decreased (0.646673 --> 0.646657).  Saving model ...\n","[  32/5000] train_loss: 0.64657 valid_loss: 0.64664\n","Validation loss decreased (0.646657 --> 0.646640).  Saving model ...\n","[  33/5000] train_loss: 0.64655 valid_loss: 0.64662\n","Validation loss decreased (0.646640 --> 0.646624).  Saving model ...\n","[  34/5000] train_loss: 0.64653 valid_loss: 0.64661\n","Validation loss decreased (0.646624 --> 0.646608).  Saving model ...\n","[  35/5000] train_loss: 0.64651 valid_loss: 0.64659\n","Validation loss decreased (0.646608 --> 0.646592).  Saving model ...\n","[  36/5000] train_loss: 0.64649 valid_loss: 0.64658\n","Validation loss decreased (0.646592 --> 0.646575).  Saving model ...\n","[  37/5000] train_loss: 0.64648 valid_loss: 0.64656\n","Validation loss decreased (0.646575 --> 0.646558).  Saving model ...\n","[  38/5000] train_loss: 0.64647 valid_loss: 0.64654\n","Validation loss decreased (0.646558 --> 0.646542).  Saving model ...\n","[  39/5000] train_loss: 0.64644 valid_loss: 0.64652\n","Validation loss decreased (0.646542 --> 0.646524).  Saving model ...\n","[  40/5000] train_loss: 0.64642 valid_loss: 0.64651\n","Validation loss decreased (0.646524 --> 0.646507).  Saving model ...\n","[  41/5000] train_loss: 0.64640 valid_loss: 0.64649\n","Validation loss decreased (0.646507 --> 0.646489).  Saving model ...\n","[  42/5000] train_loss: 0.64639 valid_loss: 0.64647\n","Validation loss decreased (0.646489 --> 0.646472).  Saving model ...\n","[  43/5000] train_loss: 0.64635 valid_loss: 0.64645\n","Validation loss decreased (0.646472 --> 0.646453).  Saving model ...\n","[  44/5000] train_loss: 0.64634 valid_loss: 0.64644\n","Validation loss decreased (0.646453 --> 0.646435).  Saving model ...\n","[  45/5000] train_loss: 0.64633 valid_loss: 0.64642\n","Validation loss decreased (0.646435 --> 0.646416).  Saving model ...\n","[  46/5000] train_loss: 0.64629 valid_loss: 0.64640\n","Validation loss decreased (0.646416 --> 0.646398).  Saving model ...\n","[  47/5000] train_loss: 0.64629 valid_loss: 0.64638\n","Validation loss decreased (0.646398 --> 0.646379).  Saving model ...\n","[  48/5000] train_loss: 0.64627 valid_loss: 0.64636\n","Validation loss decreased (0.646379 --> 0.646359).  Saving model ...\n","[  49/5000] train_loss: 0.64624 valid_loss: 0.64634\n","Validation loss decreased (0.646359 --> 0.646340).  Saving model ...\n","[  50/5000] train_loss: 0.64622 valid_loss: 0.64632\n","Validation loss decreased (0.646340 --> 0.646320).  Saving model ...\n","[  51/5000] train_loss: 0.64619 valid_loss: 0.64630\n","Validation loss decreased (0.646320 --> 0.646300).  Saving model ...\n","[  52/5000] train_loss: 0.64619 valid_loss: 0.64628\n","Validation loss decreased (0.646300 --> 0.646280).  Saving model ...\n","[  53/5000] train_loss: 0.64614 valid_loss: 0.64626\n","Validation loss decreased (0.646280 --> 0.646259).  Saving model ...\n","[  54/5000] train_loss: 0.64613 valid_loss: 0.64624\n","Validation loss decreased (0.646259 --> 0.646238).  Saving model ...\n","[  55/5000] train_loss: 0.64610 valid_loss: 0.64622\n","Validation loss decreased (0.646238 --> 0.646216).  Saving model ...\n","[  56/5000] train_loss: 0.64607 valid_loss: 0.64619\n","Validation loss decreased (0.646216 --> 0.646195).  Saving model ...\n","[  57/5000] train_loss: 0.64606 valid_loss: 0.64617\n","Validation loss decreased (0.646195 --> 0.646173).  Saving model ...\n","[  58/5000] train_loss: 0.64604 valid_loss: 0.64615\n","Validation loss decreased (0.646173 --> 0.646150).  Saving model ...\n","[  59/5000] train_loss: 0.64602 valid_loss: 0.64613\n","Validation loss decreased (0.646150 --> 0.646128).  Saving model ...\n","[  60/5000] train_loss: 0.64598 valid_loss: 0.64610\n","Validation loss decreased (0.646128 --> 0.646105).  Saving model ...\n","[  61/5000] train_loss: 0.64598 valid_loss: 0.64608\n","Validation loss decreased (0.646105 --> 0.646082).  Saving model ...\n","[  62/5000] train_loss: 0.64593 valid_loss: 0.64606\n","Validation loss decreased (0.646082 --> 0.646058).  Saving model ...\n","[  63/5000] train_loss: 0.64591 valid_loss: 0.64603\n","Validation loss decreased (0.646058 --> 0.646033).  Saving model ...\n","[  64/5000] train_loss: 0.64587 valid_loss: 0.64601\n","Validation loss decreased (0.646033 --> 0.646008).  Saving model ...\n","[  65/5000] train_loss: 0.64585 valid_loss: 0.64598\n","Validation loss decreased (0.646008 --> 0.645983).  Saving model ...\n","[  66/5000] train_loss: 0.64584 valid_loss: 0.64596\n","Validation loss decreased (0.645983 --> 0.645958).  Saving model ...\n","[  67/5000] train_loss: 0.64579 valid_loss: 0.64593\n","Validation loss decreased (0.645958 --> 0.645932).  Saving model ...\n","[  68/5000] train_loss: 0.64578 valid_loss: 0.64591\n","Validation loss decreased (0.645932 --> 0.645906).  Saving model ...\n","[  69/5000] train_loss: 0.64576 valid_loss: 0.64588\n","Validation loss decreased (0.645906 --> 0.645879).  Saving model ...\n","[  70/5000] train_loss: 0.64572 valid_loss: 0.64585\n","Validation loss decreased (0.645879 --> 0.645851).  Saving model ...\n","[  71/5000] train_loss: 0.64568 valid_loss: 0.64582\n","Validation loss decreased (0.645851 --> 0.645823).  Saving model ...\n","[  72/5000] train_loss: 0.64566 valid_loss: 0.64579\n","Validation loss decreased (0.645823 --> 0.645795).  Saving model ...\n","[  73/5000] train_loss: 0.64562 valid_loss: 0.64577\n","Validation loss decreased (0.645795 --> 0.645766).  Saving model ...\n","[  74/5000] train_loss: 0.64561 valid_loss: 0.64574\n","Validation loss decreased (0.645766 --> 0.645736).  Saving model ...\n","[  75/5000] train_loss: 0.64555 valid_loss: 0.64571\n","Validation loss decreased (0.645736 --> 0.645706).  Saving model ...\n","[  76/5000] train_loss: 0.64551 valid_loss: 0.64568\n","Validation loss decreased (0.645706 --> 0.645675).  Saving model ...\n","[  77/5000] train_loss: 0.64549 valid_loss: 0.64564\n","Validation loss decreased (0.645675 --> 0.645644).  Saving model ...\n","[  78/5000] train_loss: 0.64545 valid_loss: 0.64561\n","Validation loss decreased (0.645644 --> 0.645612).  Saving model ...\n","[  79/5000] train_loss: 0.64543 valid_loss: 0.64558\n","Validation loss decreased (0.645612 --> 0.645579).  Saving model ...\n","[  80/5000] train_loss: 0.64540 valid_loss: 0.64555\n","Validation loss decreased (0.645579 --> 0.645546).  Saving model ...\n","[  81/5000] train_loss: 0.64535 valid_loss: 0.64551\n","Validation loss decreased (0.645546 --> 0.645512).  Saving model ...\n","[  82/5000] train_loss: 0.64530 valid_loss: 0.64548\n","Validation loss decreased (0.645512 --> 0.645477).  Saving model ...\n","[  83/5000] train_loss: 0.64526 valid_loss: 0.64544\n","Validation loss decreased (0.645477 --> 0.645441).  Saving model ...\n","[  84/5000] train_loss: 0.64523 valid_loss: 0.64541\n","Validation loss decreased (0.645441 --> 0.645405).  Saving model ...\n","[  85/5000] train_loss: 0.64520 valid_loss: 0.64537\n","Validation loss decreased (0.645405 --> 0.645368).  Saving model ...\n","[  86/5000] train_loss: 0.64516 valid_loss: 0.64533\n","Validation loss decreased (0.645368 --> 0.645331).  Saving model ...\n","[  87/5000] train_loss: 0.64513 valid_loss: 0.64529\n","Validation loss decreased (0.645331 --> 0.645292).  Saving model ...\n","[  88/5000] train_loss: 0.64506 valid_loss: 0.64525\n","Validation loss decreased (0.645292 --> 0.645252).  Saving model ...\n","[  89/5000] train_loss: 0.64502 valid_loss: 0.64521\n","Validation loss decreased (0.645252 --> 0.645212).  Saving model ...\n","[  90/5000] train_loss: 0.64499 valid_loss: 0.64517\n","Validation loss decreased (0.645212 --> 0.645171).  Saving model ...\n","[  91/5000] train_loss: 0.64493 valid_loss: 0.64513\n","Validation loss decreased (0.645171 --> 0.645128).  Saving model ...\n","[  92/5000] train_loss: 0.64489 valid_loss: 0.64509\n","Validation loss decreased (0.645128 --> 0.645085).  Saving model ...\n","[  93/5000] train_loss: 0.64486 valid_loss: 0.64504\n","Validation loss decreased (0.645085 --> 0.645041).  Saving model ...\n","[  94/5000] train_loss: 0.64480 valid_loss: 0.64500\n","Validation loss decreased (0.645041 --> 0.644996).  Saving model ...\n","[  95/5000] train_loss: 0.64476 valid_loss: 0.64495\n","Validation loss decreased (0.644996 --> 0.644950).  Saving model ...\n","[  96/5000] train_loss: 0.64470 valid_loss: 0.64490\n","Validation loss decreased (0.644950 --> 0.644902).  Saving model ...\n","[  97/5000] train_loss: 0.64464 valid_loss: 0.64485\n","Validation loss decreased (0.644902 --> 0.644854).  Saving model ...\n","[  98/5000] train_loss: 0.64457 valid_loss: 0.64480\n","Validation loss decreased (0.644854 --> 0.644803).  Saving model ...\n","[  99/5000] train_loss: 0.64454 valid_loss: 0.64475\n","Validation loss decreased (0.644803 --> 0.644752).  Saving model ...\n","[ 100/5000] train_loss: 0.64448 valid_loss: 0.64470\n","Validation loss decreased (0.644752 --> 0.644700).  Saving model ...\n","[ 101/5000] train_loss: 0.64444 valid_loss: 0.64465\n","Validation loss decreased (0.644700 --> 0.644647).  Saving model ...\n","[ 102/5000] train_loss: 0.64438 valid_loss: 0.64459\n","Validation loss decreased (0.644647 --> 0.644592).  Saving model ...\n","[ 103/5000] train_loss: 0.64431 valid_loss: 0.64454\n","Validation loss decreased (0.644592 --> 0.644535).  Saving model ...\n","[ 104/5000] train_loss: 0.64424 valid_loss: 0.64448\n","Validation loss decreased (0.644535 --> 0.644477).  Saving model ...\n","[ 105/5000] train_loss: 0.64418 valid_loss: 0.64442\n","Validation loss decreased (0.644477 --> 0.644417).  Saving model ...\n","[ 106/5000] train_loss: 0.64411 valid_loss: 0.64436\n","Validation loss decreased (0.644417 --> 0.644356).  Saving model ...\n","[ 107/5000] train_loss: 0.64406 valid_loss: 0.64429\n","Validation loss decreased (0.644356 --> 0.644294).  Saving model ...\n","[ 108/5000] train_loss: 0.64396 valid_loss: 0.64423\n","Validation loss decreased (0.644294 --> 0.644229).  Saving model ...\n","[ 109/5000] train_loss: 0.64389 valid_loss: 0.64416\n","Validation loss decreased (0.644229 --> 0.644162).  Saving model ...\n","[ 110/5000] train_loss: 0.64382 valid_loss: 0.64409\n","Validation loss decreased (0.644162 --> 0.644094).  Saving model ...\n","[ 111/5000] train_loss: 0.64375 valid_loss: 0.64402\n","Validation loss decreased (0.644094 --> 0.644024).  Saving model ...\n","[ 112/5000] train_loss: 0.64367 valid_loss: 0.64395\n","Validation loss decreased (0.644024 --> 0.643951).  Saving model ...\n","[ 113/5000] train_loss: 0.64361 valid_loss: 0.64388\n","Validation loss decreased (0.643951 --> 0.643878).  Saving model ...\n","[ 114/5000] train_loss: 0.64348 valid_loss: 0.64380\n","Validation loss decreased (0.643878 --> 0.643801).  Saving model ...\n","[ 115/5000] train_loss: 0.64342 valid_loss: 0.64372\n","Validation loss decreased (0.643801 --> 0.643722).  Saving model ...\n","[ 116/5000] train_loss: 0.64334 valid_loss: 0.64364\n","Validation loss decreased (0.643722 --> 0.643641).  Saving model ...\n","[ 117/5000] train_loss: 0.64325 valid_loss: 0.64356\n","Validation loss decreased (0.643641 --> 0.643557).  Saving model ...\n","[ 118/5000] train_loss: 0.64316 valid_loss: 0.64347\n","Validation loss decreased (0.643557 --> 0.643471).  Saving model ...\n","[ 119/5000] train_loss: 0.64308 valid_loss: 0.64338\n","Validation loss decreased (0.643471 --> 0.643382).  Saving model ...\n","[ 120/5000] train_loss: 0.64298 valid_loss: 0.64329\n","Validation loss decreased (0.643382 --> 0.643291).  Saving model ...\n","[ 121/5000] train_loss: 0.64287 valid_loss: 0.64320\n","Validation loss decreased (0.643291 --> 0.643196).  Saving model ...\n","[ 122/5000] train_loss: 0.64276 valid_loss: 0.64310\n","Validation loss decreased (0.643196 --> 0.643098).  Saving model ...\n","[ 123/5000] train_loss: 0.64267 valid_loss: 0.64300\n","Validation loss decreased (0.643098 --> 0.642997).  Saving model ...\n","[ 124/5000] train_loss: 0.64253 valid_loss: 0.64289\n","Validation loss decreased (0.642997 --> 0.642893).  Saving model ...\n","[ 125/5000] train_loss: 0.64242 valid_loss: 0.64278\n","Validation loss decreased (0.642893 --> 0.642785).  Saving model ...\n","[ 126/5000] train_loss: 0.64230 valid_loss: 0.64267\n","Validation loss decreased (0.642785 --> 0.642674).  Saving model ...\n","[ 127/5000] train_loss: 0.64218 valid_loss: 0.64256\n","Validation loss decreased (0.642674 --> 0.642558).  Saving model ...\n","[ 128/5000] train_loss: 0.64209 valid_loss: 0.64244\n","Validation loss decreased (0.642558 --> 0.642440).  Saving model ...\n","[ 129/5000] train_loss: 0.64195 valid_loss: 0.64232\n","Validation loss decreased (0.642440 --> 0.642317).  Saving model ...\n","[ 130/5000] train_loss: 0.64180 valid_loss: 0.64219\n","Validation loss decreased (0.642317 --> 0.642189).  Saving model ...\n","[ 131/5000] train_loss: 0.64165 valid_loss: 0.64206\n","Validation loss decreased (0.642189 --> 0.642056).  Saving model ...\n","[ 132/5000] train_loss: 0.64151 valid_loss: 0.64192\n","Validation loss decreased (0.642056 --> 0.641919).  Saving model ...\n","[ 133/5000] train_loss: 0.64133 valid_loss: 0.64177\n","Validation loss decreased (0.641919 --> 0.641775).  Saving model ...\n","[ 134/5000] train_loss: 0.64120 valid_loss: 0.64163\n","Validation loss decreased (0.641775 --> 0.641628).  Saving model ...\n","[ 135/5000] train_loss: 0.64101 valid_loss: 0.64147\n","Validation loss decreased (0.641628 --> 0.641473).  Saving model ...\n","[ 136/5000] train_loss: 0.64088 valid_loss: 0.64131\n","Validation loss decreased (0.641473 --> 0.641313).  Saving model ...\n","[ 137/5000] train_loss: 0.64073 valid_loss: 0.64115\n","Validation loss decreased (0.641313 --> 0.641148).  Saving model ...\n","[ 138/5000] train_loss: 0.64047 valid_loss: 0.64097\n","Validation loss decreased (0.641148 --> 0.640974).  Saving model ...\n","[ 139/5000] train_loss: 0.64027 valid_loss: 0.64079\n","Validation loss decreased (0.640974 --> 0.640793).  Saving model ...\n","[ 140/5000] train_loss: 0.64006 valid_loss: 0.64060\n","Validation loss decreased (0.640793 --> 0.640604).  Saving model ...\n","[ 141/5000] train_loss: 0.63989 valid_loss: 0.64041\n","Validation loss decreased (0.640604 --> 0.640409).  Saving model ...\n","[ 142/5000] train_loss: 0.63967 valid_loss: 0.64020\n","Validation loss decreased (0.640409 --> 0.640204).  Saving model ...\n","[ 143/5000] train_loss: 0.63944 valid_loss: 0.63999\n","Validation loss decreased (0.640204 --> 0.639991).  Saving model ...\n","[ 144/5000] train_loss: 0.63922 valid_loss: 0.63977\n","Validation loss decreased (0.639991 --> 0.639768).  Saving model ...\n","[ 145/5000] train_loss: 0.63893 valid_loss: 0.63953\n","Validation loss decreased (0.639768 --> 0.639534).  Saving model ...\n","[ 146/5000] train_loss: 0.63871 valid_loss: 0.63929\n","Validation loss decreased (0.639534 --> 0.639289).  Saving model ...\n","[ 147/5000] train_loss: 0.63837 valid_loss: 0.63903\n","Validation loss decreased (0.639289 --> 0.639032).  Saving model ...\n","[ 148/5000] train_loss: 0.63810 valid_loss: 0.63876\n","Validation loss decreased (0.639032 --> 0.638764).  Saving model ...\n","[ 149/5000] train_loss: 0.63781 valid_loss: 0.63848\n","Validation loss decreased (0.638764 --> 0.638481).  Saving model ...\n","[ 150/5000] train_loss: 0.63750 valid_loss: 0.63818\n","Validation loss decreased (0.638481 --> 0.638184).  Saving model ...\n","[ 151/5000] train_loss: 0.63717 valid_loss: 0.63787\n","Validation loss decreased (0.638184 --> 0.637871).  Saving model ...\n","[ 152/5000] train_loss: 0.63681 valid_loss: 0.63754\n","Validation loss decreased (0.637871 --> 0.637541).  Saving model ...\n","[ 153/5000] train_loss: 0.63646 valid_loss: 0.63719\n","Validation loss decreased (0.637541 --> 0.637195).  Saving model ...\n","[ 154/5000] train_loss: 0.63603 valid_loss: 0.63683\n","Validation loss decreased (0.637195 --> 0.636828).  Saving model ...\n","[ 155/5000] train_loss: 0.63561 valid_loss: 0.63644\n","Validation loss decreased (0.636828 --> 0.636441).  Saving model ...\n","[ 156/5000] train_loss: 0.63520 valid_loss: 0.63603\n","Validation loss decreased (0.636441 --> 0.636033).  Saving model ...\n","[ 157/5000] train_loss: 0.63475 valid_loss: 0.63560\n","Validation loss decreased (0.636033 --> 0.635596).  Saving model ...\n","[ 158/5000] train_loss: 0.63429 valid_loss: 0.63514\n","Validation loss decreased (0.635596 --> 0.635138).  Saving model ...\n","[ 159/5000] train_loss: 0.63368 valid_loss: 0.63465\n","Validation loss decreased (0.635138 --> 0.634646).  Saving model ...\n","[ 160/5000] train_loss: 0.63317 valid_loss: 0.63412\n","Validation loss decreased (0.634646 --> 0.634125).  Saving model ...\n","[ 161/5000] train_loss: 0.63256 valid_loss: 0.63357\n","Validation loss decreased (0.634125 --> 0.633567).  Saving model ...\n","[ 162/5000] train_loss: 0.63194 valid_loss: 0.63297\n","Validation loss decreased (0.633567 --> 0.632972).  Saving model ...\n","[ 163/5000] train_loss: 0.63129 valid_loss: 0.63234\n","Validation loss decreased (0.632972 --> 0.632337).  Saving model ...\n","[ 164/5000] train_loss: 0.63053 valid_loss: 0.63165\n","Validation loss decreased (0.632337 --> 0.631650).  Saving model ...\n","[ 165/5000] train_loss: 0.62977 valid_loss: 0.63092\n","Validation loss decreased (0.631650 --> 0.630917).  Saving model ...\n","[ 166/5000] train_loss: 0.62892 valid_loss: 0.63012\n","Validation loss decreased (0.630917 --> 0.630123).  Saving model ...\n","[ 167/5000] train_loss: 0.62809 valid_loss: 0.62927\n","Validation loss decreased (0.630123 --> 0.629266).  Saving model ...\n","[ 168/5000] train_loss: 0.62701 valid_loss: 0.62834\n","Validation loss decreased (0.629266 --> 0.628337).  Saving model ...\n","[ 169/5000] train_loss: 0.62601 valid_loss: 0.62733\n","Validation loss decreased (0.628337 --> 0.627326).  Saving model ...\n","[ 170/5000] train_loss: 0.62490 valid_loss: 0.62622\n","Validation loss decreased (0.627326 --> 0.626224).  Saving model ...\n","[ 171/5000] train_loss: 0.62357 valid_loss: 0.62501\n","Validation loss decreased (0.626224 --> 0.625015).  Saving model ...\n","[ 172/5000] train_loss: 0.62220 valid_loss: 0.62369\n","Validation loss decreased (0.625015 --> 0.623694).  Saving model ...\n","[ 173/5000] train_loss: 0.62080 valid_loss: 0.62223\n","Validation loss decreased (0.623694 --> 0.622231).  Saving model ...\n","[ 174/5000] train_loss: 0.61900 valid_loss: 0.62061\n","Validation loss decreased (0.622231 --> 0.620607).  Saving model ...\n","[ 175/5000] train_loss: 0.61720 valid_loss: 0.61880\n","Validation loss decreased (0.620607 --> 0.618802).  Saving model ...\n","[ 176/5000] train_loss: 0.61511 valid_loss: 0.61679\n","Validation loss decreased (0.618802 --> 0.616788).  Saving model ...\n","[ 177/5000] train_loss: 0.61286 valid_loss: 0.61452\n","Validation loss decreased (0.616788 --> 0.614517).  Saving model ...\n","[ 178/5000] train_loss: 0.61026 valid_loss: 0.61195\n","Validation loss decreased (0.614517 --> 0.611948).  Saving model ...\n","[ 179/5000] train_loss: 0.60738 valid_loss: 0.60904\n","Validation loss decreased (0.611948 --> 0.609036).  Saving model ...\n","[ 180/5000] train_loss: 0.60399 valid_loss: 0.60570\n","Validation loss decreased (0.609036 --> 0.605699).  Saving model ...\n","[ 181/5000] train_loss: 0.60025 valid_loss: 0.60188\n","Validation loss decreased (0.605699 --> 0.601878).  Saving model ...\n","[ 182/5000] train_loss: 0.59598 valid_loss: 0.59750\n","Validation loss decreased (0.601878 --> 0.597501).  Saving model ...\n","[ 183/5000] train_loss: 0.59116 valid_loss: 0.59246\n","Validation loss decreased (0.597501 --> 0.592455).  Saving model ...\n","[ 184/5000] train_loss: 0.58526 valid_loss: 0.58666\n","Validation loss decreased (0.592455 --> 0.586659).  Saving model ...\n","[ 185/5000] train_loss: 0.57870 valid_loss: 0.58005\n","Validation loss decreased (0.586659 --> 0.580047).  Saving model ...\n","[ 186/5000] train_loss: 0.57103 valid_loss: 0.57260\n","Validation loss decreased (0.580047 --> 0.572597).  Saving model ...\n","[ 187/5000] train_loss: 0.56254 valid_loss: 0.56442\n","Validation loss decreased (0.572597 --> 0.564422).  Saving model ...\n","[ 188/5000] train_loss: 0.55308 valid_loss: 0.55576\n","Validation loss decreased (0.564422 --> 0.555761).  Saving model ...\n","[ 189/5000] train_loss: 0.54326 valid_loss: 0.54707\n","Validation loss decreased (0.555761 --> 0.547067).  Saving model ...\n","[ 190/5000] train_loss: 0.53297 valid_loss: 0.53880\n","Validation loss decreased (0.547067 --> 0.538803).  Saving model ...\n","[ 191/5000] train_loss: 0.52425 valid_loss: 0.53153\n","Validation loss decreased (0.538803 --> 0.531532).  Saving model ...\n","[ 192/5000] train_loss: 0.51619 valid_loss: 0.52536\n","Validation loss decreased (0.531532 --> 0.525357).  Saving model ...\n","[ 193/5000] train_loss: 0.50962 valid_loss: 0.51995\n","Validation loss decreased (0.525357 --> 0.519950).  Saving model ...\n","[ 194/5000] train_loss: 0.50460 valid_loss: 0.51523\n","Validation loss decreased (0.519950 --> 0.515227).  Saving model ...\n","[ 195/5000] train_loss: 0.50008 valid_loss: 0.51085\n","Validation loss decreased (0.515227 --> 0.510848).  Saving model ...\n","[ 196/5000] train_loss: 0.49646 valid_loss: 0.50637\n","Validation loss decreased (0.510848 --> 0.506366).  Saving model ...\n","[ 197/5000] train_loss: 0.49321 valid_loss: 0.50241\n","Validation loss decreased (0.506366 --> 0.502407).  Saving model ...\n","[ 198/5000] train_loss: 0.49044 valid_loss: 0.49842\n","Validation loss decreased (0.502407 --> 0.498423).  Saving model ...\n","[ 199/5000] train_loss: 0.48779 valid_loss: 0.49450\n","Validation loss decreased (0.498423 --> 0.494500).  Saving model ...\n","[ 200/5000] train_loss: 0.48523 valid_loss: 0.49104\n","Validation loss decreased (0.494500 --> 0.491036).  Saving model ...\n","[ 201/5000] train_loss: 0.48267 valid_loss: 0.48784\n","Validation loss decreased (0.491036 --> 0.487842).  Saving model ...\n","[ 202/5000] train_loss: 0.48075 valid_loss: 0.48478\n","Validation loss decreased (0.487842 --> 0.484775).  Saving model ...\n","[ 203/5000] train_loss: 0.47883 valid_loss: 0.48195\n","Validation loss decreased (0.484775 --> 0.481951).  Saving model ...\n","[ 204/5000] train_loss: 0.47709 valid_loss: 0.47926\n","Validation loss decreased (0.481951 --> 0.479261).  Saving model ...\n","[ 205/5000] train_loss: 0.47501 valid_loss: 0.47691\n","Validation loss decreased (0.479261 --> 0.476914).  Saving model ...\n","[ 206/5000] train_loss: 0.47377 valid_loss: 0.47461\n","Validation loss decreased (0.476914 --> 0.474608).  Saving model ...\n","[ 207/5000] train_loss: 0.47155 valid_loss: 0.47260\n","Validation loss decreased (0.474608 --> 0.472598).  Saving model ...\n","[ 208/5000] train_loss: 0.47043 valid_loss: 0.47068\n","Validation loss decreased (0.472598 --> 0.470677).  Saving model ...\n","[ 209/5000] train_loss: 0.46918 valid_loss: 0.46890\n","Validation loss decreased (0.470677 --> 0.468903).  Saving model ...\n","[ 210/5000] train_loss: 0.46787 valid_loss: 0.46728\n","Validation loss decreased (0.468903 --> 0.467277).  Saving model ...\n","[ 211/5000] train_loss: 0.46654 valid_loss: 0.46580\n","Validation loss decreased (0.467277 --> 0.465801).  Saving model ...\n","[ 212/5000] train_loss: 0.46528 valid_loss: 0.46442\n","Validation loss decreased (0.465801 --> 0.464425).  Saving model ...\n","[ 213/5000] train_loss: 0.46360 valid_loss: 0.46320\n","Validation loss decreased (0.464425 --> 0.463196).  Saving model ...\n","[ 214/5000] train_loss: 0.46250 valid_loss: 0.46208\n","Validation loss decreased (0.463196 --> 0.462080).  Saving model ...\n","[ 215/5000] train_loss: 0.46125 valid_loss: 0.46120\n","Validation loss decreased (0.462080 --> 0.461202).  Saving model ...\n","[ 216/5000] train_loss: 0.45941 valid_loss: 0.46015\n","Validation loss decreased (0.461202 --> 0.460154).  Saving model ...\n","[ 217/5000] train_loss: 0.45847 valid_loss: 0.45942\n","Validation loss decreased (0.460154 --> 0.459417).  Saving model ...\n","[ 218/5000] train_loss: 0.45707 valid_loss: 0.45846\n","Validation loss decreased (0.459417 --> 0.458462).  Saving model ...\n","[ 219/5000] train_loss: 0.45602 valid_loss: 0.45798\n","Validation loss decreased (0.458462 --> 0.457982).  Saving model ...\n","[ 220/5000] train_loss: 0.45477 valid_loss: 0.45718\n","Validation loss decreased (0.457982 --> 0.457185).  Saving model ...\n","[ 221/5000] train_loss: 0.45348 valid_loss: 0.45660\n","Validation loss decreased (0.457185 --> 0.456600).  Saving model ...\n","[ 222/5000] train_loss: 0.45248 valid_loss: 0.45598\n","Validation loss decreased (0.456600 --> 0.455983).  Saving model ...\n","[ 223/5000] train_loss: 0.45094 valid_loss: 0.45557\n","Validation loss decreased (0.455983 --> 0.455573).  Saving model ...\n","[ 224/5000] train_loss: 0.44927 valid_loss: 0.45514\n","Validation loss decreased (0.455573 --> 0.455141).  Saving model ...\n","[ 225/5000] train_loss: 0.44843 valid_loss: 0.45452\n","Validation loss decreased (0.455141 --> 0.454523).  Saving model ...\n","[ 226/5000] train_loss: 0.44685 valid_loss: 0.45375\n","Validation loss decreased (0.454523 --> 0.453752).  Saving model ...\n","[ 227/5000] train_loss: 0.44592 valid_loss: 0.45306\n","Validation loss decreased (0.453752 --> 0.453058).  Saving model ...\n","[ 228/5000] train_loss: 0.44387 valid_loss: 0.45263\n","Validation loss decreased (0.453058 --> 0.452626).  Saving model ...\n","[ 229/5000] train_loss: 0.44261 valid_loss: 0.45212\n","Validation loss decreased (0.452626 --> 0.452119).  Saving model ...\n","[ 230/5000] train_loss: 0.44156 valid_loss: 0.45172\n","Validation loss decreased (0.452119 --> 0.451718).  Saving model ...\n","[ 231/5000] train_loss: 0.43997 valid_loss: 0.45050\n","Validation loss decreased (0.451718 --> 0.450501).  Saving model ...\n","[ 232/5000] train_loss: 0.43891 valid_loss: 0.45023\n","Validation loss decreased (0.450501 --> 0.450229).  Saving model ...\n","[ 233/5000] train_loss: 0.43734 valid_loss: 0.44885\n","Validation loss decreased (0.450229 --> 0.448848).  Saving model ...\n","[ 234/5000] train_loss: 0.43652 valid_loss: 0.44890\n","EarlyStopping counter: 1 out of 300\n","[ 235/5000] train_loss: 0.43462 valid_loss: 0.44749\n","Validation loss decreased (0.448848 --> 0.447487).  Saving model ...\n","[ 236/5000] train_loss: 0.43416 valid_loss: 0.44699\n","Validation loss decreased (0.447487 --> 0.446993).  Saving model ...\n","[ 237/5000] train_loss: 0.43231 valid_loss: 0.44606\n","Validation loss decreased (0.446993 --> 0.446057).  Saving model ...\n","[ 238/5000] train_loss: 0.43148 valid_loss: 0.44474\n","Validation loss decreased (0.446057 --> 0.444743).  Saving model ...\n","[ 239/5000] train_loss: 0.43024 valid_loss: 0.44400\n","Validation loss decreased (0.444743 --> 0.444001).  Saving model ...\n","[ 240/5000] train_loss: 0.42899 valid_loss: 0.44249\n","Validation loss decreased (0.444001 --> 0.442486).  Saving model ...\n","[ 241/5000] train_loss: 0.42798 valid_loss: 0.44205\n","Validation loss decreased (0.442486 --> 0.442047).  Saving model ...\n","[ 242/5000] train_loss: 0.42733 valid_loss: 0.44087\n","Validation loss decreased (0.442047 --> 0.440874).  Saving model ...\n","[ 243/5000] train_loss: 0.42549 valid_loss: 0.44064\n","Validation loss decreased (0.440874 --> 0.440641).  Saving model ...\n","[ 244/5000] train_loss: 0.42509 valid_loss: 0.43874\n","Validation loss decreased (0.440641 --> 0.438739).  Saving model ...\n","[ 245/5000] train_loss: 0.42378 valid_loss: 0.43734\n","Validation loss decreased (0.438739 --> 0.437341).  Saving model ...\n","[ 246/5000] train_loss: 0.42364 valid_loss: 0.43727\n","Validation loss decreased (0.437341 --> 0.437274).  Saving model ...\n","[ 247/5000] train_loss: 0.42270 valid_loss: 0.43578\n","Validation loss decreased (0.437274 --> 0.435781).  Saving model ...\n","[ 248/5000] train_loss: 0.42125 valid_loss: 0.43434\n","Validation loss decreased (0.435781 --> 0.434336).  Saving model ...\n","[ 249/5000] train_loss: 0.42213 valid_loss: 0.43348\n","Validation loss decreased (0.434336 --> 0.433484).  Saving model ...\n","[ 250/5000] train_loss: 0.42040 valid_loss: 0.43255\n","Validation loss decreased (0.433484 --> 0.432548).  Saving model ...\n","[ 251/5000] train_loss: 0.42006 valid_loss: 0.43125\n","Validation loss decreased (0.432548 --> 0.431251).  Saving model ...\n","[ 252/5000] train_loss: 0.41886 valid_loss: 0.43065\n","Validation loss decreased (0.431251 --> 0.430647).  Saving model ...\n","[ 253/5000] train_loss: 0.41879 valid_loss: 0.42949\n","Validation loss decreased (0.430647 --> 0.429495).  Saving model ...\n","[ 254/5000] train_loss: 0.41786 valid_loss: 0.42894\n","Validation loss decreased (0.429495 --> 0.428942).  Saving model ...\n","[ 255/5000] train_loss: 0.41765 valid_loss: 0.42736\n","Validation loss decreased (0.428942 --> 0.427363).  Saving model ...\n","[ 256/5000] train_loss: 0.41715 valid_loss: 0.42660\n","Validation loss decreased (0.427363 --> 0.426596).  Saving model ...\n","[ 257/5000] train_loss: 0.41590 valid_loss: 0.42607\n","Validation loss decreased (0.426596 --> 0.426073).  Saving model ...\n","[ 258/5000] train_loss: 0.41542 valid_loss: 0.42521\n","Validation loss decreased (0.426073 --> 0.425205).  Saving model ...\n","[ 259/5000] train_loss: 0.41568 valid_loss: 0.42426\n","Validation loss decreased (0.425205 --> 0.424256).  Saving model ...\n","[ 260/5000] train_loss: 0.41456 valid_loss: 0.42333\n","Validation loss decreased (0.424256 --> 0.423328).  Saving model ...\n","[ 261/5000] train_loss: 0.41422 valid_loss: 0.42284\n","Validation loss decreased (0.423328 --> 0.422837).  Saving model ...\n","[ 262/5000] train_loss: 0.41422 valid_loss: 0.42217\n","Validation loss decreased (0.422837 --> 0.422168).  Saving model ...\n","[ 263/5000] train_loss: 0.41407 valid_loss: 0.42163\n","Validation loss decreased (0.422168 --> 0.421632).  Saving model ...\n","[ 264/5000] train_loss: 0.41280 valid_loss: 0.42103\n","Validation loss decreased (0.421632 --> 0.421033).  Saving model ...\n","[ 265/5000] train_loss: 0.41329 valid_loss: 0.42065\n","Validation loss decreased (0.421033 --> 0.420652).  Saving model ...\n","[ 266/5000] train_loss: 0.41329 valid_loss: 0.42015\n","Validation loss decreased (0.420652 --> 0.420151).  Saving model ...\n","[ 267/5000] train_loss: 0.41161 valid_loss: 0.41966\n","Validation loss decreased (0.420151 --> 0.419665).  Saving model ...\n","[ 268/5000] train_loss: 0.41264 valid_loss: 0.41925\n","Validation loss decreased (0.419665 --> 0.419249).  Saving model ...\n","[ 269/5000] train_loss: 0.41034 valid_loss: 0.41871\n","Validation loss decreased (0.419249 --> 0.418713).  Saving model ...\n","[ 270/5000] train_loss: 0.41113 valid_loss: 0.41843\n","Validation loss decreased (0.418713 --> 0.418430).  Saving model ...\n","[ 271/5000] train_loss: 0.41064 valid_loss: 0.41803\n","Validation loss decreased (0.418430 --> 0.418026).  Saving model ...\n","[ 272/5000] train_loss: 0.41154 valid_loss: 0.41772\n","Validation loss decreased (0.418026 --> 0.417718).  Saving model ...\n","[ 273/5000] train_loss: 0.41002 valid_loss: 0.41768\n","Validation loss decreased (0.417718 --> 0.417684).  Saving model ...\n","[ 274/5000] train_loss: 0.40948 valid_loss: 0.41716\n","Validation loss decreased (0.417684 --> 0.417163).  Saving model ...\n","[ 275/5000] train_loss: 0.40964 valid_loss: 0.41695\n","Validation loss decreased (0.417163 --> 0.416945).  Saving model ...\n","[ 276/5000] train_loss: 0.40877 valid_loss: 0.41673\n","Validation loss decreased (0.416945 --> 0.416728).  Saving model ...\n","[ 277/5000] train_loss: 0.40908 valid_loss: 0.41661\n","Validation loss decreased (0.416728 --> 0.416614).  Saving model ...\n","[ 278/5000] train_loss: 0.40877 valid_loss: 0.41666\n","EarlyStopping counter: 1 out of 300\n","[ 279/5000] train_loss: 0.40773 valid_loss: 0.41609\n","Validation loss decreased (0.416614 --> 0.416092).  Saving model ...\n","[ 280/5000] train_loss: 0.40832 valid_loss: 0.41634\n","EarlyStopping counter: 1 out of 300\n","[ 281/5000] train_loss: 0.40798 valid_loss: 0.41605\n","Validation loss decreased (0.416092 --> 0.416045).  Saving model ...\n","[ 282/5000] train_loss: 0.40744 valid_loss: 0.41555\n","Validation loss decreased (0.416045 --> 0.415554).  Saving model ...\n","[ 283/5000] train_loss: 0.40703 valid_loss: 0.41594\n","EarlyStopping counter: 1 out of 300\n","[ 284/5000] train_loss: 0.40697 valid_loss: 0.41534\n","Validation loss decreased (0.415554 --> 0.415340).  Saving model ...\n","[ 285/5000] train_loss: 0.40611 valid_loss: 0.41583\n","EarlyStopping counter: 1 out of 300\n","[ 286/5000] train_loss: 0.40583 valid_loss: 0.41584\n","EarlyStopping counter: 2 out of 300\n","[ 287/5000] train_loss: 0.40578 valid_loss: 0.41546\n","EarlyStopping counter: 3 out of 300\n","[ 288/5000] train_loss: 0.40624 valid_loss: 0.41554\n","EarlyStopping counter: 4 out of 300\n","[ 289/5000] train_loss: 0.40549 valid_loss: 0.41518\n","Validation loss decreased (0.415340 --> 0.415178).  Saving model ...\n","[ 290/5000] train_loss: 0.40568 valid_loss: 0.41576\n","EarlyStopping counter: 1 out of 300\n","[ 291/5000] train_loss: 0.40503 valid_loss: 0.41499\n","Validation loss decreased (0.415178 --> 0.414990).  Saving model ...\n","[ 292/5000] train_loss: 0.40476 valid_loss: 0.41512\n","EarlyStopping counter: 1 out of 300\n","[ 293/5000] train_loss: 0.40479 valid_loss: 0.41552\n","EarlyStopping counter: 2 out of 300\n","[ 294/5000] train_loss: 0.40396 valid_loss: 0.41528\n","EarlyStopping counter: 3 out of 300\n","[ 295/5000] train_loss: 0.40400 valid_loss: 0.41681\n","EarlyStopping counter: 4 out of 300\n","[ 296/5000] train_loss: 0.40414 valid_loss: 0.41542\n","EarlyStopping counter: 5 out of 300\n","[ 297/5000] train_loss: 0.40329 valid_loss: 0.41527\n","EarlyStopping counter: 6 out of 300\n","[ 298/5000] train_loss: 0.40275 valid_loss: 0.41542\n","EarlyStopping counter: 7 out of 300\n","[ 299/5000] train_loss: 0.40352 valid_loss: 0.41605\n","EarlyStopping counter: 8 out of 300\n","[ 300/5000] train_loss: 0.40258 valid_loss: 0.41535\n","EarlyStopping counter: 9 out of 300\n","[ 301/5000] train_loss: 0.40308 valid_loss: 0.41576\n","EarlyStopping counter: 10 out of 300\n","[ 302/5000] train_loss: 0.40231 valid_loss: 0.41619\n","EarlyStopping counter: 11 out of 300\n","[ 303/5000] train_loss: 0.40242 valid_loss: 0.41626\n","EarlyStopping counter: 12 out of 300\n","[ 304/5000] train_loss: 0.40140 valid_loss: 0.41583\n","EarlyStopping counter: 13 out of 300\n","[ 305/5000] train_loss: 0.40176 valid_loss: 0.41654\n","EarlyStopping counter: 14 out of 300\n","[ 306/5000] train_loss: 0.40197 valid_loss: 0.41597\n","EarlyStopping counter: 15 out of 300\n","[ 307/5000] train_loss: 0.40179 valid_loss: 0.41593\n","EarlyStopping counter: 16 out of 300\n","[ 308/5000] train_loss: 0.40108 valid_loss: 0.41552\n","EarlyStopping counter: 17 out of 300\n","[ 309/5000] train_loss: 0.40088 valid_loss: 0.41513\n","EarlyStopping counter: 18 out of 300\n","[ 310/5000] train_loss: 0.40108 valid_loss: 0.41586\n","EarlyStopping counter: 19 out of 300\n","[ 311/5000] train_loss: 0.40040 valid_loss: 0.41549\n","EarlyStopping counter: 20 out of 300\n","[ 312/5000] train_loss: 0.40015 valid_loss: 0.41557\n","EarlyStopping counter: 21 out of 300\n","[ 313/5000] train_loss: 0.40001 valid_loss: 0.41551\n","EarlyStopping counter: 22 out of 300\n","[ 314/5000] train_loss: 0.39947 valid_loss: 0.41540\n","EarlyStopping counter: 23 out of 300\n","[ 315/5000] train_loss: 0.40025 valid_loss: 0.41532\n","EarlyStopping counter: 24 out of 300\n","[ 316/5000] train_loss: 0.40019 valid_loss: 0.41559\n","EarlyStopping counter: 25 out of 300\n","[ 317/5000] train_loss: 0.39962 valid_loss: 0.41517\n","EarlyStopping counter: 26 out of 300\n","[ 318/5000] train_loss: 0.39820 valid_loss: 0.41528\n","EarlyStopping counter: 27 out of 300\n","[ 319/5000] train_loss: 0.39923 valid_loss: 0.41517\n","EarlyStopping counter: 28 out of 300\n","[ 320/5000] train_loss: 0.39885 valid_loss: 0.41561\n","EarlyStopping counter: 29 out of 300\n","[ 321/5000] train_loss: 0.39845 valid_loss: 0.41531\n","EarlyStopping counter: 30 out of 300\n","[ 322/5000] train_loss: 0.39833 valid_loss: 0.41505\n","EarlyStopping counter: 31 out of 300\n","[ 323/5000] train_loss: 0.39790 valid_loss: 0.41569\n","EarlyStopping counter: 32 out of 300\n","[ 324/5000] train_loss: 0.39816 valid_loss: 0.41449\n","Validation loss decreased (0.414990 --> 0.414489).  Saving model ...\n","[ 325/5000] train_loss: 0.39771 valid_loss: 0.41492\n","EarlyStopping counter: 1 out of 300\n","[ 326/5000] train_loss: 0.39713 valid_loss: 0.41482\n","EarlyStopping counter: 2 out of 300\n","[ 327/5000] train_loss: 0.39697 valid_loss: 0.41487\n","EarlyStopping counter: 3 out of 300\n","[ 328/5000] train_loss: 0.39802 valid_loss: 0.41470\n","EarlyStopping counter: 4 out of 300\n","[ 329/5000] train_loss: 0.39642 valid_loss: 0.41411\n","Validation loss decreased (0.414489 --> 0.414109).  Saving model ...\n","[ 330/5000] train_loss: 0.39592 valid_loss: 0.41472\n","EarlyStopping counter: 1 out of 300\n","[ 331/5000] train_loss: 0.39701 valid_loss: 0.41438\n","EarlyStopping counter: 2 out of 300\n","[ 332/5000] train_loss: 0.39645 valid_loss: 0.41505\n","EarlyStopping counter: 3 out of 300\n","[ 333/5000] train_loss: 0.39652 valid_loss: 0.41415\n","EarlyStopping counter: 4 out of 300\n","[ 334/5000] train_loss: 0.39633 valid_loss: 0.41448\n","EarlyStopping counter: 5 out of 300\n","[ 335/5000] train_loss: 0.39583 valid_loss: 0.41446\n","EarlyStopping counter: 6 out of 300\n","[ 336/5000] train_loss: 0.39578 valid_loss: 0.41315\n","Validation loss decreased (0.414109 --> 0.413147).  Saving model ...\n","[ 337/5000] train_loss: 0.39547 valid_loss: 0.41453\n","EarlyStopping counter: 1 out of 300\n","[ 338/5000] train_loss: 0.39544 valid_loss: 0.41338\n","EarlyStopping counter: 2 out of 300\n","[ 339/5000] train_loss: 0.39441 valid_loss: 0.41326\n","EarlyStopping counter: 3 out of 300\n","[ 340/5000] train_loss: 0.39521 valid_loss: 0.41361\n","EarlyStopping counter: 4 out of 300\n","[ 341/5000] train_loss: 0.39455 valid_loss: 0.41285\n","Validation loss decreased (0.413147 --> 0.412847).  Saving model ...\n","[ 342/5000] train_loss: 0.39452 valid_loss: 0.41277\n","Validation loss decreased (0.412847 --> 0.412772).  Saving model ...\n","[ 343/5000] train_loss: 0.39509 valid_loss: 0.41253\n","Validation loss decreased (0.412772 --> 0.412531).  Saving model ...\n","[ 344/5000] train_loss: 0.39424 valid_loss: 0.41265\n","EarlyStopping counter: 1 out of 300\n","[ 345/5000] train_loss: 0.39370 valid_loss: 0.41275\n","EarlyStopping counter: 2 out of 300\n","[ 346/5000] train_loss: 0.39354 valid_loss: 0.41324\n","EarlyStopping counter: 3 out of 300\n","[ 347/5000] train_loss: 0.39381 valid_loss: 0.41243\n","Validation loss decreased (0.412531 --> 0.412431).  Saving model ...\n","[ 348/5000] train_loss: 0.39364 valid_loss: 0.41175\n","Validation loss decreased (0.412431 --> 0.411747).  Saving model ...\n","[ 349/5000] train_loss: 0.39289 valid_loss: 0.41185\n","EarlyStopping counter: 1 out of 300\n","[ 350/5000] train_loss: 0.39351 valid_loss: 0.41065\n","Validation loss decreased (0.411747 --> 0.410646).  Saving model ...\n","[ 351/5000] train_loss: 0.39332 valid_loss: 0.41199\n","EarlyStopping counter: 1 out of 300\n","[ 352/5000] train_loss: 0.39233 valid_loss: 0.41154\n","EarlyStopping counter: 2 out of 300\n","[ 353/5000] train_loss: 0.39302 valid_loss: 0.41165\n","EarlyStopping counter: 3 out of 300\n","[ 354/5000] train_loss: 0.39199 valid_loss: 0.41160\n","EarlyStopping counter: 4 out of 300\n","[ 355/5000] train_loss: 0.39250 valid_loss: 0.41193\n","EarlyStopping counter: 5 out of 300\n","[ 356/5000] train_loss: 0.39184 valid_loss: 0.41143\n","EarlyStopping counter: 6 out of 300\n","[ 357/5000] train_loss: 0.39152 valid_loss: 0.41129\n","EarlyStopping counter: 7 out of 300\n","[ 358/5000] train_loss: 0.39210 valid_loss: 0.41027\n","Validation loss decreased (0.410646 --> 0.410266).  Saving model ...\n","[ 359/5000] train_loss: 0.39127 valid_loss: 0.40979\n","Validation loss decreased (0.410266 --> 0.409794).  Saving model ...\n","[ 360/5000] train_loss: 0.39111 valid_loss: 0.41029\n","EarlyStopping counter: 1 out of 300\n","[ 361/5000] train_loss: 0.39135 valid_loss: 0.40974\n","Validation loss decreased (0.409794 --> 0.409737).  Saving model ...\n","[ 362/5000] train_loss: 0.39078 valid_loss: 0.41008\n","EarlyStopping counter: 1 out of 300\n","[ 363/5000] train_loss: 0.39052 valid_loss: 0.40971\n","Validation loss decreased (0.409737 --> 0.409712).  Saving model ...\n","[ 364/5000] train_loss: 0.39163 valid_loss: 0.40959\n","Validation loss decreased (0.409712 --> 0.409592).  Saving model ...\n","[ 365/5000] train_loss: 0.39108 valid_loss: 0.40946\n","Validation loss decreased (0.409592 --> 0.409457).  Saving model ...\n","[ 366/5000] train_loss: 0.39010 valid_loss: 0.40943\n","Validation loss decreased (0.409457 --> 0.409428).  Saving model ...\n","[ 367/5000] train_loss: 0.39068 valid_loss: 0.40828\n","Validation loss decreased (0.409428 --> 0.408276).  Saving model ...\n","[ 368/5000] train_loss: 0.38977 valid_loss: 0.40957\n","EarlyStopping counter: 1 out of 300\n","[ 369/5000] train_loss: 0.38962 valid_loss: 0.40881\n","EarlyStopping counter: 2 out of 300\n","[ 370/5000] train_loss: 0.38893 valid_loss: 0.40962\n","EarlyStopping counter: 3 out of 300\n","[ 371/5000] train_loss: 0.38964 valid_loss: 0.40827\n","Validation loss decreased (0.408276 --> 0.408266).  Saving model ...\n","[ 372/5000] train_loss: 0.38952 valid_loss: 0.40835\n","EarlyStopping counter: 1 out of 300\n","[ 373/5000] train_loss: 0.38904 valid_loss: 0.40852\n","EarlyStopping counter: 2 out of 300\n","[ 374/5000] train_loss: 0.38969 valid_loss: 0.40778\n","Validation loss decreased (0.408266 --> 0.407779).  Saving model ...\n","[ 375/5000] train_loss: 0.38878 valid_loss: 0.40767\n","Validation loss decreased (0.407779 --> 0.407666).  Saving model ...\n","[ 376/5000] train_loss: 0.38819 valid_loss: 0.40845\n","EarlyStopping counter: 1 out of 300\n","[ 377/5000] train_loss: 0.38926 valid_loss: 0.40745\n","Validation loss decreased (0.407666 --> 0.407447).  Saving model ...\n","[ 378/5000] train_loss: 0.38842 valid_loss: 0.40770\n","EarlyStopping counter: 1 out of 300\n","[ 379/5000] train_loss: 0.38793 valid_loss: 0.40745\n","EarlyStopping counter: 2 out of 300\n","[ 380/5000] train_loss: 0.38769 valid_loss: 0.40762\n","EarlyStopping counter: 3 out of 300\n","[ 381/5000] train_loss: 0.38808 valid_loss: 0.40695\n","Validation loss decreased (0.407447 --> 0.406948).  Saving model ...\n","[ 382/5000] train_loss: 0.38696 valid_loss: 0.40694\n","Validation loss decreased (0.406948 --> 0.406944).  Saving model ...\n","[ 383/5000] train_loss: 0.38749 valid_loss: 0.40778\n","EarlyStopping counter: 1 out of 300\n","[ 384/5000] train_loss: 0.38782 valid_loss: 0.40734\n","EarlyStopping counter: 2 out of 300\n","[ 385/5000] train_loss: 0.38675 valid_loss: 0.40611\n","Validation loss decreased (0.406944 --> 0.406113).  Saving model ...\n","[ 386/5000] train_loss: 0.38693 valid_loss: 0.40629\n","EarlyStopping counter: 1 out of 300\n","[ 387/5000] train_loss: 0.38677 valid_loss: 0.40683\n","EarlyStopping counter: 2 out of 300\n","[ 388/5000] train_loss: 0.38712 valid_loss: 0.40570\n","Validation loss decreased (0.406113 --> 0.405703).  Saving model ...\n","[ 389/5000] train_loss: 0.38690 valid_loss: 0.40541\n","Validation loss decreased (0.405703 --> 0.405410).  Saving model ...\n","[ 390/5000] train_loss: 0.38680 valid_loss: 0.40492\n","Validation loss decreased (0.405410 --> 0.404923).  Saving model ...\n","[ 391/5000] train_loss: 0.38637 valid_loss: 0.40516\n","EarlyStopping counter: 1 out of 300\n","[ 392/5000] train_loss: 0.38598 valid_loss: 0.40546\n","EarlyStopping counter: 2 out of 300\n","[ 393/5000] train_loss: 0.38578 valid_loss: 0.40445\n","Validation loss decreased (0.404923 --> 0.404453).  Saving model ...\n","[ 394/5000] train_loss: 0.38599 valid_loss: 0.40542\n","EarlyStopping counter: 1 out of 300\n","[ 395/5000] train_loss: 0.38510 valid_loss: 0.40482\n","EarlyStopping counter: 2 out of 300\n","[ 396/5000] train_loss: 0.38565 valid_loss: 0.40436\n","Validation loss decreased (0.404453 --> 0.404362).  Saving model ...\n","[ 397/5000] train_loss: 0.38571 valid_loss: 0.40437\n","EarlyStopping counter: 1 out of 300\n","[ 398/5000] train_loss: 0.38424 valid_loss: 0.40460\n","EarlyStopping counter: 2 out of 300\n","[ 399/5000] train_loss: 0.38527 valid_loss: 0.40469\n","EarlyStopping counter: 3 out of 300\n","[ 400/5000] train_loss: 0.38482 valid_loss: 0.40360\n","Validation loss decreased (0.404362 --> 0.403600).  Saving model ...\n","[ 401/5000] train_loss: 0.38400 valid_loss: 0.40411\n","EarlyStopping counter: 1 out of 300\n","[ 402/5000] train_loss: 0.38427 valid_loss: 0.40426\n","EarlyStopping counter: 2 out of 300\n","[ 403/5000] train_loss: 0.38337 valid_loss: 0.40394\n","EarlyStopping counter: 3 out of 300\n","[ 404/5000] train_loss: 0.38437 valid_loss: 0.40352\n","Validation loss decreased (0.403600 --> 0.403516).  Saving model ...\n","[ 405/5000] train_loss: 0.38371 valid_loss: 0.40299\n","Validation loss decreased (0.403516 --> 0.402991).  Saving model ...\n","[ 406/5000] train_loss: 0.38323 valid_loss: 0.40328\n","EarlyStopping counter: 1 out of 300\n","[ 407/5000] train_loss: 0.38323 valid_loss: 0.40294\n","Validation loss decreased (0.402991 --> 0.402939).  Saving model ...\n","[ 408/5000] train_loss: 0.38338 valid_loss: 0.40267\n","Validation loss decreased (0.402939 --> 0.402673).  Saving model ...\n","[ 409/5000] train_loss: 0.38237 valid_loss: 0.40234\n","Validation loss decreased (0.402673 --> 0.402338).  Saving model ...\n","[ 410/5000] train_loss: 0.38329 valid_loss: 0.40193\n","Validation loss decreased (0.402338 --> 0.401926).  Saving model ...\n","[ 411/5000] train_loss: 0.38181 valid_loss: 0.40263\n","EarlyStopping counter: 1 out of 300\n","[ 412/5000] train_loss: 0.38203 valid_loss: 0.40172\n","Validation loss decreased (0.401926 --> 0.401717).  Saving model ...\n","[ 413/5000] train_loss: 0.38247 valid_loss: 0.40137\n","Validation loss decreased (0.401717 --> 0.401369).  Saving model ...\n","[ 414/5000] train_loss: 0.38235 valid_loss: 0.40155\n","EarlyStopping counter: 1 out of 300\n","[ 415/5000] train_loss: 0.38133 valid_loss: 0.40119\n","Validation loss decreased (0.401369 --> 0.401190).  Saving model ...\n","[ 416/5000] train_loss: 0.38161 valid_loss: 0.40083\n","Validation loss decreased (0.401190 --> 0.400832).  Saving model ...\n","[ 417/5000] train_loss: 0.38158 valid_loss: 0.40067\n","Validation loss decreased (0.400832 --> 0.400668).  Saving model ...\n","[ 418/5000] train_loss: 0.38153 valid_loss: 0.40130\n","EarlyStopping counter: 1 out of 300\n","[ 419/5000] train_loss: 0.38143 valid_loss: 0.40054\n","Validation loss decreased (0.400668 --> 0.400543).  Saving model ...\n","[ 420/5000] train_loss: 0.38122 valid_loss: 0.40053\n","Validation loss decreased (0.400543 --> 0.400533).  Saving model ...\n","[ 421/5000] train_loss: 0.38094 valid_loss: 0.39986\n","Validation loss decreased (0.400533 --> 0.399857).  Saving model ...\n","[ 422/5000] train_loss: 0.38018 valid_loss: 0.40045\n","EarlyStopping counter: 1 out of 300\n","[ 423/5000] train_loss: 0.38108 valid_loss: 0.40042\n","EarlyStopping counter: 2 out of 300\n","[ 424/5000] train_loss: 0.38029 valid_loss: 0.39937\n","Validation loss decreased (0.399857 --> 0.399375).  Saving model ...\n","[ 425/5000] train_loss: 0.38025 valid_loss: 0.39913\n","Validation loss decreased (0.399375 --> 0.399127).  Saving model ...\n","[ 426/5000] train_loss: 0.38002 valid_loss: 0.39963\n","EarlyStopping counter: 1 out of 300\n","[ 427/5000] train_loss: 0.38021 valid_loss: 0.39927\n","EarlyStopping counter: 2 out of 300\n","[ 428/5000] train_loss: 0.38014 valid_loss: 0.39994\n","EarlyStopping counter: 3 out of 300\n","[ 429/5000] train_loss: 0.37905 valid_loss: 0.39897\n","Validation loss decreased (0.399127 --> 0.398972).  Saving model ...\n","[ 430/5000] train_loss: 0.37908 valid_loss: 0.39917\n","EarlyStopping counter: 1 out of 300\n","[ 431/5000] train_loss: 0.37819 valid_loss: 0.39875\n","Validation loss decreased (0.398972 --> 0.398747).  Saving model ...\n","[ 432/5000] train_loss: 0.37905 valid_loss: 0.39884\n","EarlyStopping counter: 1 out of 300\n","[ 433/5000] train_loss: 0.37807 valid_loss: 0.39824\n","Validation loss decreased (0.398747 --> 0.398243).  Saving model ...\n","[ 434/5000] train_loss: 0.37817 valid_loss: 0.39783\n","Validation loss decreased (0.398243 --> 0.397830).  Saving model ...\n","[ 435/5000] train_loss: 0.37849 valid_loss: 0.39747\n","Validation loss decreased (0.397830 --> 0.397470).  Saving model ...\n","[ 436/5000] train_loss: 0.37762 valid_loss: 0.39728\n","Validation loss decreased (0.397470 --> 0.397281).  Saving model ...\n","[ 437/5000] train_loss: 0.37807 valid_loss: 0.39835\n","EarlyStopping counter: 1 out of 300\n","[ 438/5000] train_loss: 0.37784 valid_loss: 0.39729\n","EarlyStopping counter: 2 out of 300\n","[ 439/5000] train_loss: 0.37753 valid_loss: 0.39682\n","Validation loss decreased (0.397281 --> 0.396820).  Saving model ...\n","[ 440/5000] train_loss: 0.37747 valid_loss: 0.39729\n","EarlyStopping counter: 1 out of 300\n","[ 441/5000] train_loss: 0.37708 valid_loss: 0.39674\n","Validation loss decreased (0.396820 --> 0.396738).  Saving model ...\n","[ 442/5000] train_loss: 0.37623 valid_loss: 0.39652\n","Validation loss decreased (0.396738 --> 0.396517).  Saving model ...\n","[ 443/5000] train_loss: 0.37618 valid_loss: 0.39705\n","EarlyStopping counter: 1 out of 300\n","[ 444/5000] train_loss: 0.37588 valid_loss: 0.39628\n","Validation loss decreased (0.396517 --> 0.396277).  Saving model ...\n","[ 445/5000] train_loss: 0.37758 valid_loss: 0.39576\n","Validation loss decreased (0.396277 --> 0.395758).  Saving model ...\n","[ 446/5000] train_loss: 0.37575 valid_loss: 0.39556\n","Validation loss decreased (0.395758 --> 0.395559).  Saving model ...\n","[ 447/5000] train_loss: 0.37557 valid_loss: 0.39498\n","Validation loss decreased (0.395559 --> 0.394983).  Saving model ...\n","[ 448/5000] train_loss: 0.37464 valid_loss: 0.39532\n","EarlyStopping counter: 1 out of 300\n","[ 449/5000] train_loss: 0.37509 valid_loss: 0.39469\n","Validation loss decreased (0.394983 --> 0.394685).  Saving model ...\n","[ 450/5000] train_loss: 0.37468 valid_loss: 0.39492\n","EarlyStopping counter: 1 out of 300\n","[ 451/5000] train_loss: 0.37447 valid_loss: 0.39513\n","EarlyStopping counter: 2 out of 300\n","[ 452/5000] train_loss: 0.37437 valid_loss: 0.39448\n","Validation loss decreased (0.394685 --> 0.394480).  Saving model ...\n","[ 453/5000] train_loss: 0.37244 valid_loss: 0.39422\n","Validation loss decreased (0.394480 --> 0.394221).  Saving model ...\n","[ 454/5000] train_loss: 0.37275 valid_loss: 0.39404\n","Validation loss decreased (0.394221 --> 0.394039).  Saving model ...\n","[ 455/5000] train_loss: 0.37394 valid_loss: 0.39366\n","Validation loss decreased (0.394039 --> 0.393662).  Saving model ...\n","[ 456/5000] train_loss: 0.37245 valid_loss: 0.39357\n","Validation loss decreased (0.393662 --> 0.393569).  Saving model ...\n","[ 457/5000] train_loss: 0.37276 valid_loss: 0.39444\n","EarlyStopping counter: 1 out of 300\n","[ 458/5000] train_loss: 0.37240 valid_loss: 0.39357\n","EarlyStopping counter: 2 out of 300\n","[ 459/5000] train_loss: 0.37213 valid_loss: 0.39404\n","EarlyStopping counter: 3 out of 300\n","[ 460/5000] train_loss: 0.37154 valid_loss: 0.39270\n","Validation loss decreased (0.393569 --> 0.392702).  Saving model ...\n","[ 461/5000] train_loss: 0.37154 valid_loss: 0.39330\n","EarlyStopping counter: 1 out of 300\n","[ 462/5000] train_loss: 0.37113 valid_loss: 0.39261\n","Validation loss decreased (0.392702 --> 0.392608).  Saving model ...\n","[ 463/5000] train_loss: 0.36997 valid_loss: 0.39239\n","Validation loss decreased (0.392608 --> 0.392392).  Saving model ...\n","[ 464/5000] train_loss: 0.36951 valid_loss: 0.39257\n","EarlyStopping counter: 1 out of 300\n","[ 465/5000] train_loss: 0.37113 valid_loss: 0.39150\n","Validation loss decreased (0.392392 --> 0.391505).  Saving model ...\n","[ 466/5000] train_loss: 0.37033 valid_loss: 0.39241\n","EarlyStopping counter: 1 out of 300\n","[ 467/5000] train_loss: 0.37047 valid_loss: 0.39122\n","Validation loss decreased (0.391505 --> 0.391218).  Saving model ...\n","[ 468/5000] train_loss: 0.36951 valid_loss: 0.39115\n","Validation loss decreased (0.391218 --> 0.391150).  Saving model ...\n","[ 469/5000] train_loss: 0.36852 valid_loss: 0.39119\n","EarlyStopping counter: 1 out of 300\n","[ 470/5000] train_loss: 0.36978 valid_loss: 0.39154\n","EarlyStopping counter: 2 out of 300\n","[ 471/5000] train_loss: 0.36916 valid_loss: 0.39019\n","Validation loss decreased (0.391150 --> 0.390186).  Saving model ...\n","[ 472/5000] train_loss: 0.36723 valid_loss: 0.38956\n","Validation loss decreased (0.390186 --> 0.389561).  Saving model ...\n","[ 473/5000] train_loss: 0.37002 valid_loss: 0.38882\n","Validation loss decreased (0.389561 --> 0.388823).  Saving model ...\n","[ 474/5000] train_loss: 0.36910 valid_loss: 0.38993\n","EarlyStopping counter: 1 out of 300\n","[ 475/5000] train_loss: 0.36681 valid_loss: 0.39156\n","EarlyStopping counter: 2 out of 300\n","[ 476/5000] train_loss: 0.36788 valid_loss: 0.38902\n","EarlyStopping counter: 3 out of 300\n","[ 477/5000] train_loss: 0.36520 valid_loss: 0.38965\n","EarlyStopping counter: 4 out of 300\n","[ 478/5000] train_loss: 0.36607 valid_loss: 0.39012\n","EarlyStopping counter: 5 out of 300\n","[ 479/5000] train_loss: 0.36548 valid_loss: 0.38836\n","Validation loss decreased (0.388823 --> 0.388360).  Saving model ...\n","[ 480/5000] train_loss: 0.36503 valid_loss: 0.38927\n","EarlyStopping counter: 1 out of 300\n","[ 481/5000] train_loss: 0.36476 valid_loss: 0.38853\n","EarlyStopping counter: 2 out of 300\n","[ 482/5000] train_loss: 0.36527 valid_loss: 0.38863\n","EarlyStopping counter: 3 out of 300\n","[ 483/5000] train_loss: 0.36585 valid_loss: 0.38789\n","Validation loss decreased (0.388360 --> 0.387895).  Saving model ...\n","[ 484/5000] train_loss: 0.36273 valid_loss: 0.38915\n","EarlyStopping counter: 1 out of 300\n","[ 485/5000] train_loss: 0.36315 valid_loss: 0.39062\n","EarlyStopping counter: 2 out of 300\n","[ 486/5000] train_loss: 0.36537 valid_loss: 0.38668\n","Validation loss decreased (0.387895 --> 0.386679).  Saving model ...\n","[ 487/5000] train_loss: 0.36156 valid_loss: 0.38782\n","EarlyStopping counter: 1 out of 300\n","[ 488/5000] train_loss: 0.36236 valid_loss: 0.38668\n","EarlyStopping counter: 2 out of 300\n","[ 489/5000] train_loss: 0.36221 valid_loss: 0.38671\n","EarlyStopping counter: 3 out of 300\n","[ 490/5000] train_loss: 0.36318 valid_loss: 0.38563\n","Validation loss decreased (0.386679 --> 0.385634).  Saving model ...\n","[ 491/5000] train_loss: 0.36165 valid_loss: 0.38597\n","EarlyStopping counter: 1 out of 300\n","[ 492/5000] train_loss: 0.36218 valid_loss: 0.38509\n","Validation loss decreased (0.385634 --> 0.385088).  Saving model ...\n","[ 493/5000] train_loss: 0.36076 valid_loss: 0.38839\n","EarlyStopping counter: 1 out of 300\n","[ 494/5000] train_loss: 0.36037 valid_loss: 0.38514\n","EarlyStopping counter: 2 out of 300\n","[ 495/5000] train_loss: 0.36228 valid_loss: 0.38586\n","EarlyStopping counter: 3 out of 300\n","[ 496/5000] train_loss: 0.35993 valid_loss: 0.38432\n","Validation loss decreased (0.385088 --> 0.384323).  Saving model ...\n","[ 497/5000] train_loss: 0.36177 valid_loss: 0.38453\n","EarlyStopping counter: 1 out of 300\n","[ 498/5000] train_loss: 0.36057 valid_loss: 0.38506\n","EarlyStopping counter: 2 out of 300\n","[ 499/5000] train_loss: 0.35937 valid_loss: 0.38647\n","EarlyStopping counter: 3 out of 300\n","[ 500/5000] train_loss: 0.35798 valid_loss: 0.38700\n","EarlyStopping counter: 4 out of 300\n","[ 501/5000] train_loss: 0.35775 valid_loss: 0.38721\n","EarlyStopping counter: 5 out of 300\n","[ 502/5000] train_loss: 0.35801 valid_loss: 0.38998\n","EarlyStopping counter: 6 out of 300\n","[ 503/5000] train_loss: 0.35834 valid_loss: 0.38616\n","EarlyStopping counter: 7 out of 300\n","[ 504/5000] train_loss: 0.35652 valid_loss: 0.38772\n","EarlyStopping counter: 8 out of 300\n","[ 505/5000] train_loss: 0.35708 valid_loss: 0.38626\n","EarlyStopping counter: 9 out of 300\n","[ 506/5000] train_loss: 0.35695 valid_loss: 0.38551\n","EarlyStopping counter: 10 out of 300\n","[ 507/5000] train_loss: 0.35697 valid_loss: 0.38731\n","EarlyStopping counter: 11 out of 300\n","[ 508/5000] train_loss: 0.35609 valid_loss: 0.38518\n","EarlyStopping counter: 12 out of 300\n","[ 509/5000] train_loss: 0.35509 valid_loss: 0.38919\n","EarlyStopping counter: 13 out of 300\n","[ 510/5000] train_loss: 0.35673 valid_loss: 0.38623\n","EarlyStopping counter: 14 out of 300\n","[ 511/5000] train_loss: 0.35658 valid_loss: 0.38517\n","EarlyStopping counter: 15 out of 300\n","[ 512/5000] train_loss: 0.35352 valid_loss: 0.38867\n","EarlyStopping counter: 16 out of 300\n","[ 513/5000] train_loss: 0.35228 valid_loss: 0.38740\n","EarlyStopping counter: 17 out of 300\n","[ 514/5000] train_loss: 0.35626 valid_loss: 0.38759\n","EarlyStopping counter: 18 out of 300\n","[ 515/5000] train_loss: 0.35315 valid_loss: 0.38868\n","EarlyStopping counter: 19 out of 300\n","[ 516/5000] train_loss: 0.35229 valid_loss: 0.38656\n","EarlyStopping counter: 20 out of 300\n","[ 517/5000] train_loss: 0.35496 valid_loss: 0.38609\n","EarlyStopping counter: 21 out of 300\n","[ 518/5000] train_loss: 0.35353 valid_loss: 0.38577\n","EarlyStopping counter: 22 out of 300\n","[ 519/5000] train_loss: 0.35218 valid_loss: 0.38668\n","EarlyStopping counter: 23 out of 300\n","[ 520/5000] train_loss: 0.35335 valid_loss: 0.38782\n","EarlyStopping counter: 24 out of 300\n","[ 521/5000] train_loss: 0.35177 valid_loss: 0.38547\n","EarlyStopping counter: 25 out of 300\n","[ 522/5000] train_loss: 0.35234 valid_loss: 0.38450\n","EarlyStopping counter: 26 out of 300\n","[ 523/5000] train_loss: 0.35128 valid_loss: 0.38447\n","EarlyStopping counter: 27 out of 300\n","[ 524/5000] train_loss: 0.35316 valid_loss: 0.38440\n","EarlyStopping counter: 28 out of 300\n","[ 525/5000] train_loss: 0.35060 valid_loss: 0.38636\n","EarlyStopping counter: 29 out of 300\n","[ 526/5000] train_loss: 0.35231 valid_loss: 0.38653\n","EarlyStopping counter: 30 out of 300\n","[ 527/5000] train_loss: 0.35335 valid_loss: 0.38306\n","Validation loss decreased (0.384323 --> 0.383058).  Saving model ...\n","[ 528/5000] train_loss: 0.35028 valid_loss: 0.38468\n","EarlyStopping counter: 1 out of 300\n","[ 529/5000] train_loss: 0.34992 valid_loss: 0.38352\n","EarlyStopping counter: 2 out of 300\n","[ 530/5000] train_loss: 0.34900 valid_loss: 0.38237\n","Validation loss decreased (0.383058 --> 0.382369).  Saving model ...\n","[ 531/5000] train_loss: 0.34963 valid_loss: 0.38419\n","EarlyStopping counter: 1 out of 300\n","[ 532/5000] train_loss: 0.35023 valid_loss: 0.38521\n","EarlyStopping counter: 2 out of 300\n","[ 533/5000] train_loss: 0.35069 valid_loss: 0.38406\n","EarlyStopping counter: 3 out of 300\n","[ 534/5000] train_loss: 0.35081 valid_loss: 0.38082\n","Validation loss decreased (0.382369 --> 0.380825).  Saving model ...\n","[ 535/5000] train_loss: 0.34694 valid_loss: 0.38360\n","EarlyStopping counter: 1 out of 300\n","[ 536/5000] train_loss: 0.34728 valid_loss: 0.38301\n","EarlyStopping counter: 2 out of 300\n","[ 537/5000] train_loss: 0.34773 valid_loss: 0.38280\n","EarlyStopping counter: 3 out of 300\n","[ 538/5000] train_loss: 0.34745 valid_loss: 0.38399\n","EarlyStopping counter: 4 out of 300\n","[ 539/5000] train_loss: 0.34496 valid_loss: 0.38325\n","EarlyStopping counter: 5 out of 300\n","[ 540/5000] train_loss: 0.34950 valid_loss: 0.38449\n","EarlyStopping counter: 6 out of 300\n","[ 541/5000] train_loss: 0.34873 valid_loss: 0.38258\n","EarlyStopping counter: 7 out of 300\n","[ 542/5000] train_loss: 0.34689 valid_loss: 0.38282\n","EarlyStopping counter: 8 out of 300\n","[ 543/5000] train_loss: 0.34634 valid_loss: 0.38269\n","EarlyStopping counter: 9 out of 300\n","[ 544/5000] train_loss: 0.34681 valid_loss: 0.37961\n","Validation loss decreased (0.380825 --> 0.379606).  Saving model ...\n","[ 545/5000] train_loss: 0.34682 valid_loss: 0.38091\n","EarlyStopping counter: 1 out of 300\n","[ 546/5000] train_loss: 0.34678 valid_loss: 0.38170\n","EarlyStopping counter: 2 out of 300\n","[ 547/5000] train_loss: 0.34714 valid_loss: 0.38164\n","EarlyStopping counter: 3 out of 300\n","[ 548/5000] train_loss: 0.34571 valid_loss: 0.38000\n","EarlyStopping counter: 4 out of 300\n","[ 549/5000] train_loss: 0.34327 valid_loss: 0.38153\n","EarlyStopping counter: 5 out of 300\n","[ 550/5000] train_loss: 0.34449 valid_loss: 0.38284\n","EarlyStopping counter: 6 out of 300\n","[ 551/5000] train_loss: 0.34358 valid_loss: 0.38244\n","EarlyStopping counter: 7 out of 300\n","[ 552/5000] train_loss: 0.34328 valid_loss: 0.38096\n","EarlyStopping counter: 8 out of 300\n","[ 553/5000] train_loss: 0.34452 valid_loss: 0.38057\n","EarlyStopping counter: 9 out of 300\n","[ 554/5000] train_loss: 0.34370 valid_loss: 0.38093\n","EarlyStopping counter: 10 out of 300\n","[ 555/5000] train_loss: 0.34642 valid_loss: 0.38023\n","EarlyStopping counter: 11 out of 300\n","[ 556/5000] train_loss: 0.34261 valid_loss: 0.37956\n","Validation loss decreased (0.379606 --> 0.379561).  Saving model ...\n","[ 557/5000] train_loss: 0.34306 valid_loss: 0.38133\n","EarlyStopping counter: 1 out of 300\n","[ 558/5000] train_loss: 0.34406 valid_loss: 0.37963\n","EarlyStopping counter: 2 out of 300\n","[ 559/5000] train_loss: 0.34464 valid_loss: 0.37873\n","Validation loss decreased (0.379561 --> 0.378730).  Saving model ...\n","[ 560/5000] train_loss: 0.34258 valid_loss: 0.38061\n","EarlyStopping counter: 1 out of 300\n","[ 561/5000] train_loss: 0.34228 valid_loss: 0.38125\n","EarlyStopping counter: 2 out of 300\n","[ 562/5000] train_loss: 0.34282 valid_loss: 0.37890\n","EarlyStopping counter: 3 out of 300\n","[ 563/5000] train_loss: 0.34167 valid_loss: 0.37984\n","EarlyStopping counter: 4 out of 300\n","[ 564/5000] train_loss: 0.34223 valid_loss: 0.37892\n","EarlyStopping counter: 5 out of 300\n","[ 565/5000] train_loss: 0.34194 valid_loss: 0.38175\n","EarlyStopping counter: 6 out of 300\n","[ 566/5000] train_loss: 0.34180 valid_loss: 0.37928\n","EarlyStopping counter: 7 out of 300\n","[ 567/5000] train_loss: 0.34333 valid_loss: 0.37818\n","Validation loss decreased (0.378730 --> 0.378177).  Saving model ...\n","[ 568/5000] train_loss: 0.34056 valid_loss: 0.38060\n","EarlyStopping counter: 1 out of 300\n","[ 569/5000] train_loss: 0.34103 valid_loss: 0.37833\n","EarlyStopping counter: 2 out of 300\n","[ 570/5000] train_loss: 0.34206 valid_loss: 0.37847\n","EarlyStopping counter: 3 out of 300\n","[ 571/5000] train_loss: 0.34054 valid_loss: 0.37896\n","EarlyStopping counter: 4 out of 300\n","[ 572/5000] train_loss: 0.33962 valid_loss: 0.37924\n","EarlyStopping counter: 5 out of 300\n","[ 573/5000] train_loss: 0.34002 valid_loss: 0.38029\n","EarlyStopping counter: 6 out of 300\n","[ 574/5000] train_loss: 0.33978 valid_loss: 0.37969\n","EarlyStopping counter: 7 out of 300\n","[ 575/5000] train_loss: 0.34168 valid_loss: 0.37761\n","Validation loss decreased (0.378177 --> 0.377606).  Saving model ...\n","[ 576/5000] train_loss: 0.33913 valid_loss: 0.37909\n","EarlyStopping counter: 1 out of 300\n","[ 577/5000] train_loss: 0.33975 valid_loss: 0.37770\n","EarlyStopping counter: 2 out of 300\n","[ 578/5000] train_loss: 0.33936 valid_loss: 0.37963\n","EarlyStopping counter: 3 out of 300\n","[ 579/5000] train_loss: 0.34088 valid_loss: 0.37723\n","Validation loss decreased (0.377606 --> 0.377232).  Saving model ...\n","[ 580/5000] train_loss: 0.33839 valid_loss: 0.37861\n","EarlyStopping counter: 1 out of 300\n","[ 581/5000] train_loss: 0.33636 valid_loss: 0.37805\n","EarlyStopping counter: 2 out of 300\n","[ 582/5000] train_loss: 0.34018 valid_loss: 0.37915\n","EarlyStopping counter: 3 out of 300\n","[ 583/5000] train_loss: 0.33861 valid_loss: 0.37853\n","EarlyStopping counter: 4 out of 300\n","[ 584/5000] train_loss: 0.33784 valid_loss: 0.37788\n","EarlyStopping counter: 5 out of 300\n","[ 585/5000] train_loss: 0.33912 valid_loss: 0.37759\n","EarlyStopping counter: 6 out of 300\n","[ 586/5000] train_loss: 0.33814 valid_loss: 0.37866\n","EarlyStopping counter: 7 out of 300\n","[ 587/5000] train_loss: 0.33919 valid_loss: 0.37717\n","Validation loss decreased (0.377232 --> 0.377173).  Saving model ...\n","[ 588/5000] train_loss: 0.33805 valid_loss: 0.37852\n","EarlyStopping counter: 1 out of 300\n","[ 589/5000] train_loss: 0.33719 valid_loss: 0.37655\n","Validation loss decreased (0.377173 --> 0.376553).  Saving model ...\n","[ 590/5000] train_loss: 0.33745 valid_loss: 0.37774\n","EarlyStopping counter: 1 out of 300\n","[ 591/5000] train_loss: 0.33789 valid_loss: 0.37725\n","EarlyStopping counter: 2 out of 300\n","[ 592/5000] train_loss: 0.33680 valid_loss: 0.37738\n","EarlyStopping counter: 3 out of 300\n","[ 593/5000] train_loss: 0.33730 valid_loss: 0.37921\n","EarlyStopping counter: 4 out of 300\n","[ 594/5000] train_loss: 0.33713 valid_loss: 0.37690\n","EarlyStopping counter: 5 out of 300\n","[ 595/5000] train_loss: 0.33537 valid_loss: 0.37639\n","Validation loss decreased (0.376553 --> 0.376386).  Saving model ...\n","[ 596/5000] train_loss: 0.33592 valid_loss: 0.37700\n","EarlyStopping counter: 1 out of 300\n","[ 597/5000] train_loss: 0.33579 valid_loss: 0.37706\n","EarlyStopping counter: 2 out of 300\n","[ 598/5000] train_loss: 0.33710 valid_loss: 0.37541\n","Validation loss decreased (0.376386 --> 0.375414).  Saving model ...\n","[ 599/5000] train_loss: 0.33862 valid_loss: 0.37721\n","EarlyStopping counter: 1 out of 300\n","[ 600/5000] train_loss: 0.33584 valid_loss: 0.37678\n","EarlyStopping counter: 2 out of 300\n","[ 601/5000] train_loss: 0.33527 valid_loss: 0.37705\n","EarlyStopping counter: 3 out of 300\n","[ 602/5000] train_loss: 0.33714 valid_loss: 0.37645\n","EarlyStopping counter: 4 out of 300\n","[ 603/5000] train_loss: 0.33565 valid_loss: 0.37648\n","EarlyStopping counter: 5 out of 300\n","[ 604/5000] train_loss: 0.33676 valid_loss: 0.37552\n","EarlyStopping counter: 6 out of 300\n","[ 605/5000] train_loss: 0.33503 valid_loss: 0.37651\n","EarlyStopping counter: 7 out of 300\n","[ 606/5000] train_loss: 0.33514 valid_loss: 0.37688\n","EarlyStopping counter: 8 out of 300\n","[ 607/5000] train_loss: 0.33614 valid_loss: 0.37694\n","EarlyStopping counter: 9 out of 300\n","[ 608/5000] train_loss: 0.33755 valid_loss: 0.37645\n","EarlyStopping counter: 10 out of 300\n","[ 609/5000] train_loss: 0.33423 valid_loss: 0.37646\n","EarlyStopping counter: 11 out of 300\n","[ 610/5000] train_loss: 0.33224 valid_loss: 0.37614\n","EarlyStopping counter: 12 out of 300\n","[ 611/5000] train_loss: 0.33405 valid_loss: 0.37595\n","EarlyStopping counter: 13 out of 300\n","[ 612/5000] train_loss: 0.33276 valid_loss: 0.37513\n","Validation loss decreased (0.375414 --> 0.375132).  Saving model ...\n","[ 613/5000] train_loss: 0.33482 valid_loss: 0.37590\n","EarlyStopping counter: 1 out of 300\n","[ 614/5000] train_loss: 0.33325 valid_loss: 0.37641\n","EarlyStopping counter: 2 out of 300\n","[ 615/5000] train_loss: 0.33442 valid_loss: 0.37576\n","EarlyStopping counter: 3 out of 300\n","[ 616/5000] train_loss: 0.33475 valid_loss: 0.37606\n","EarlyStopping counter: 4 out of 300\n","[ 617/5000] train_loss: 0.33319 valid_loss: 0.37510\n","Validation loss decreased (0.375132 --> 0.375099).  Saving model ...\n","[ 618/5000] train_loss: 0.33468 valid_loss: 0.37500\n","Validation loss decreased (0.375099 --> 0.375002).  Saving model ...\n","[ 619/5000] train_loss: 0.33281 valid_loss: 0.37567\n","EarlyStopping counter: 1 out of 300\n","[ 620/5000] train_loss: 0.33135 valid_loss: 0.37597\n","EarlyStopping counter: 2 out of 300\n","[ 621/5000] train_loss: 0.33386 valid_loss: 0.37508\n","EarlyStopping counter: 3 out of 300\n","[ 622/5000] train_loss: 0.33367 valid_loss: 0.37489\n","Validation loss decreased (0.375002 --> 0.374891).  Saving model ...\n","[ 623/5000] train_loss: 0.33152 valid_loss: 0.37545\n","EarlyStopping counter: 1 out of 300\n","[ 624/5000] train_loss: 0.33389 valid_loss: 0.37559\n","EarlyStopping counter: 2 out of 300\n","[ 625/5000] train_loss: 0.33368 valid_loss: 0.37611\n","EarlyStopping counter: 3 out of 300\n","[ 626/5000] train_loss: 0.33398 valid_loss: 0.37535\n","EarlyStopping counter: 4 out of 300\n","[ 627/5000] train_loss: 0.33213 valid_loss: 0.37519\n","EarlyStopping counter: 5 out of 300\n","[ 628/5000] train_loss: 0.33308 valid_loss: 0.37436\n","Validation loss decreased (0.374891 --> 0.374365).  Saving model ...\n","[ 629/5000] train_loss: 0.33211 valid_loss: 0.37646\n","EarlyStopping counter: 1 out of 300\n","[ 630/5000] train_loss: 0.33285 valid_loss: 0.37590\n","EarlyStopping counter: 2 out of 300\n","[ 631/5000] train_loss: 0.33137 valid_loss: 0.37610\n","EarlyStopping counter: 3 out of 300\n","[ 632/5000] train_loss: 0.33252 valid_loss: 0.37464\n","EarlyStopping counter: 4 out of 300\n","[ 633/5000] train_loss: 0.33358 valid_loss: 0.37445\n","EarlyStopping counter: 5 out of 300\n","[ 634/5000] train_loss: 0.33256 valid_loss: 0.37535\n","EarlyStopping counter: 6 out of 300\n","[ 635/5000] train_loss: 0.33168 valid_loss: 0.37523\n","EarlyStopping counter: 7 out of 300\n","[ 636/5000] train_loss: 0.33215 valid_loss: 0.37550\n","EarlyStopping counter: 8 out of 300\n","[ 637/5000] train_loss: 0.33071 valid_loss: 0.37576\n","EarlyStopping counter: 9 out of 300\n","[ 638/5000] train_loss: 0.32940 valid_loss: 0.37535\n","EarlyStopping counter: 10 out of 300\n","[ 639/5000] train_loss: 0.33262 valid_loss: 0.37498\n","EarlyStopping counter: 11 out of 300\n","[ 640/5000] train_loss: 0.33172 valid_loss: 0.37518\n","EarlyStopping counter: 12 out of 300\n","[ 641/5000] train_loss: 0.33077 valid_loss: 0.37555\n","EarlyStopping counter: 13 out of 300\n","[ 642/5000] train_loss: 0.33060 valid_loss: 0.37532\n","EarlyStopping counter: 14 out of 300\n","[ 643/5000] train_loss: 0.32997 valid_loss: 0.37562\n","EarlyStopping counter: 15 out of 300\n","[ 644/5000] train_loss: 0.33052 valid_loss: 0.37531\n","EarlyStopping counter: 16 out of 300\n","[ 645/5000] train_loss: 0.32996 valid_loss: 0.37613\n","EarlyStopping counter: 17 out of 300\n","[ 646/5000] train_loss: 0.33066 valid_loss: 0.37392\n","Validation loss decreased (0.374365 --> 0.373924).  Saving model ...\n","[ 647/5000] train_loss: 0.33101 valid_loss: 0.37533\n","EarlyStopping counter: 1 out of 300\n","[ 648/5000] train_loss: 0.33104 valid_loss: 0.37454\n","EarlyStopping counter: 2 out of 300\n","[ 649/5000] train_loss: 0.32749 valid_loss: 0.37500\n","EarlyStopping counter: 3 out of 300\n","[ 650/5000] train_loss: 0.32899 valid_loss: 0.37560\n","EarlyStopping counter: 4 out of 300\n","[ 651/5000] train_loss: 0.32944 valid_loss: 0.37578\n","EarlyStopping counter: 5 out of 300\n","[ 652/5000] train_loss: 0.32998 valid_loss: 0.37570\n","EarlyStopping counter: 6 out of 300\n","[ 653/5000] train_loss: 0.32863 valid_loss: 0.37526\n","EarlyStopping counter: 7 out of 300\n","[ 654/5000] train_loss: 0.32991 valid_loss: 0.37465\n","EarlyStopping counter: 8 out of 300\n","[ 655/5000] train_loss: 0.32943 valid_loss: 0.37403\n","EarlyStopping counter: 9 out of 300\n","[ 656/5000] train_loss: 0.32911 valid_loss: 0.37412\n","EarlyStopping counter: 10 out of 300\n","[ 657/5000] train_loss: 0.32845 valid_loss: 0.37452\n","EarlyStopping counter: 11 out of 300\n","[ 658/5000] train_loss: 0.32765 valid_loss: 0.37479\n","EarlyStopping counter: 12 out of 300\n","[ 659/5000] train_loss: 0.32826 valid_loss: 0.37512\n","EarlyStopping counter: 13 out of 300\n","[ 660/5000] train_loss: 0.32829 valid_loss: 0.37540\n","EarlyStopping counter: 14 out of 300\n","[ 661/5000] train_loss: 0.32836 valid_loss: 0.37502\n","EarlyStopping counter: 15 out of 300\n","[ 662/5000] train_loss: 0.32709 valid_loss: 0.37534\n","EarlyStopping counter: 16 out of 300\n","[ 663/5000] train_loss: 0.32767 valid_loss: 0.37518\n","EarlyStopping counter: 17 out of 300\n","[ 664/5000] train_loss: 0.32881 valid_loss: 0.37445\n","EarlyStopping counter: 18 out of 300\n","[ 665/5000] train_loss: 0.32845 valid_loss: 0.37458\n","EarlyStopping counter: 19 out of 300\n","[ 666/5000] train_loss: 0.32731 valid_loss: 0.37534\n","EarlyStopping counter: 20 out of 300\n","[ 667/5000] train_loss: 0.32840 valid_loss: 0.37520\n","EarlyStopping counter: 21 out of 300\n","[ 668/5000] train_loss: 0.32676 valid_loss: 0.37459\n","EarlyStopping counter: 22 out of 300\n","[ 669/5000] train_loss: 0.32722 valid_loss: 0.37482\n","EarlyStopping counter: 23 out of 300\n","[ 670/5000] train_loss: 0.32591 valid_loss: 0.37549\n","EarlyStopping counter: 24 out of 300\n","[ 671/5000] train_loss: 0.32752 valid_loss: 0.37465\n","EarlyStopping counter: 25 out of 300\n","[ 672/5000] train_loss: 0.32654 valid_loss: 0.37508\n","EarlyStopping counter: 26 out of 300\n","[ 673/5000] train_loss: 0.32804 valid_loss: 0.37472\n","EarlyStopping counter: 27 out of 300\n","[ 674/5000] train_loss: 0.32727 valid_loss: 0.37499\n","EarlyStopping counter: 28 out of 300\n","[ 675/5000] train_loss: 0.32562 valid_loss: 0.37501\n","EarlyStopping counter: 29 out of 300\n","[ 676/5000] train_loss: 0.32706 valid_loss: 0.37464\n","EarlyStopping counter: 30 out of 300\n","[ 677/5000] train_loss: 0.32647 valid_loss: 0.37470\n","EarlyStopping counter: 31 out of 300\n","[ 678/5000] train_loss: 0.32811 valid_loss: 0.37384\n","Validation loss decreased (0.373924 --> 0.373838).  Saving model ...\n","[ 679/5000] train_loss: 0.32716 valid_loss: 0.37400\n","EarlyStopping counter: 1 out of 300\n","[ 680/5000] train_loss: 0.32620 valid_loss: 0.37437\n","EarlyStopping counter: 2 out of 300\n","[ 681/5000] train_loss: 0.32643 valid_loss: 0.37488\n","EarlyStopping counter: 3 out of 300\n","[ 682/5000] train_loss: 0.32371 valid_loss: 0.37519\n","EarlyStopping counter: 4 out of 300\n","[ 683/5000] train_loss: 0.32565 valid_loss: 0.37473\n","EarlyStopping counter: 5 out of 300\n","[ 684/5000] train_loss: 0.32657 valid_loss: 0.37511\n","EarlyStopping counter: 6 out of 300\n","[ 685/5000] train_loss: 0.32620 valid_loss: 0.37502\n","EarlyStopping counter: 7 out of 300\n","[ 686/5000] train_loss: 0.32590 valid_loss: 0.37493\n","EarlyStopping counter: 8 out of 300\n","[ 687/5000] train_loss: 0.32836 valid_loss: 0.37398\n","EarlyStopping counter: 9 out of 300\n","[ 688/5000] train_loss: 0.32505 valid_loss: 0.37417\n","EarlyStopping counter: 10 out of 300\n","[ 689/5000] train_loss: 0.32727 valid_loss: 0.37441\n","EarlyStopping counter: 11 out of 300\n","[ 690/5000] train_loss: 0.32564 valid_loss: 0.37437\n","EarlyStopping counter: 12 out of 300\n","[ 691/5000] train_loss: 0.32356 valid_loss: 0.37499\n","EarlyStopping counter: 13 out of 300\n","[ 692/5000] train_loss: 0.32348 valid_loss: 0.37458\n","EarlyStopping counter: 14 out of 300\n","[ 693/5000] train_loss: 0.32569 valid_loss: 0.37471\n","EarlyStopping counter: 15 out of 300\n","[ 694/5000] train_loss: 0.32332 valid_loss: 0.37520\n","EarlyStopping counter: 16 out of 300\n","[ 695/5000] train_loss: 0.32485 valid_loss: 0.37437\n","EarlyStopping counter: 17 out of 300\n","[ 696/5000] train_loss: 0.32419 valid_loss: 0.37405\n","EarlyStopping counter: 18 out of 300\n","[ 697/5000] train_loss: 0.32357 valid_loss: 0.37462\n","EarlyStopping counter: 19 out of 300\n","[ 698/5000] train_loss: 0.32364 valid_loss: 0.37421\n","EarlyStopping counter: 20 out of 300\n","[ 699/5000] train_loss: 0.32572 valid_loss: 0.37485\n","EarlyStopping counter: 21 out of 300\n","[ 700/5000] train_loss: 0.32468 valid_loss: 0.37505\n","EarlyStopping counter: 22 out of 300\n","[ 701/5000] train_loss: 0.32603 valid_loss: 0.37408\n","EarlyStopping counter: 23 out of 300\n","[ 702/5000] train_loss: 0.32500 valid_loss: 0.37383\n","Validation loss decreased (0.373838 --> 0.373835).  Saving model ...\n","[ 703/5000] train_loss: 0.32381 valid_loss: 0.37431\n","EarlyStopping counter: 1 out of 300\n","[ 704/5000] train_loss: 0.32499 valid_loss: 0.37469\n","EarlyStopping counter: 2 out of 300\n","[ 705/5000] train_loss: 0.32348 valid_loss: 0.37476\n","EarlyStopping counter: 3 out of 300\n","[ 706/5000] train_loss: 0.32333 valid_loss: 0.37460\n","EarlyStopping counter: 4 out of 300\n","[ 707/5000] train_loss: 0.32413 valid_loss: 0.37477\n","EarlyStopping counter: 5 out of 300\n","[ 708/5000] train_loss: 0.32170 valid_loss: 0.37481\n","EarlyStopping counter: 6 out of 300\n","[ 709/5000] train_loss: 0.32557 valid_loss: 0.37383\n","Validation loss decreased (0.373835 --> 0.373828).  Saving model ...\n","[ 710/5000] train_loss: 0.32247 valid_loss: 0.37502\n","EarlyStopping counter: 1 out of 300\n","[ 711/5000] train_loss: 0.32238 valid_loss: 0.37396\n","EarlyStopping counter: 2 out of 300\n","[ 712/5000] train_loss: 0.32165 valid_loss: 0.37462\n","EarlyStopping counter: 3 out of 300\n","[ 713/5000] train_loss: 0.32218 valid_loss: 0.37474\n","EarlyStopping counter: 4 out of 300\n","[ 714/5000] train_loss: 0.32225 valid_loss: 0.37472\n","EarlyStopping counter: 5 out of 300\n","[ 715/5000] train_loss: 0.32403 valid_loss: 0.37408\n","EarlyStopping counter: 6 out of 300\n","[ 716/5000] train_loss: 0.32399 valid_loss: 0.37421\n","EarlyStopping counter: 7 out of 300\n","[ 717/5000] train_loss: 0.32234 valid_loss: 0.37464\n","EarlyStopping counter: 8 out of 300\n","[ 718/5000] train_loss: 0.32206 valid_loss: 0.37480\n","EarlyStopping counter: 9 out of 300\n","[ 719/5000] train_loss: 0.32379 valid_loss: 0.37497\n","EarlyStopping counter: 10 out of 300\n","[ 720/5000] train_loss: 0.32027 valid_loss: 0.37474\n","EarlyStopping counter: 11 out of 300\n","[ 721/5000] train_loss: 0.32204 valid_loss: 0.37426\n","EarlyStopping counter: 12 out of 300\n","[ 722/5000] train_loss: 0.32051 valid_loss: 0.37558\n","EarlyStopping counter: 13 out of 300\n","[ 723/5000] train_loss: 0.32308 valid_loss: 0.37490\n","EarlyStopping counter: 14 out of 300\n","[ 724/5000] train_loss: 0.32164 valid_loss: 0.37449\n","EarlyStopping counter: 15 out of 300\n","[ 725/5000] train_loss: 0.32119 valid_loss: 0.37467\n","EarlyStopping counter: 16 out of 300\n","[ 726/5000] train_loss: 0.32194 valid_loss: 0.37534\n","EarlyStopping counter: 17 out of 300\n","[ 727/5000] train_loss: 0.32049 valid_loss: 0.37580\n","EarlyStopping counter: 18 out of 300\n","[ 728/5000] train_loss: 0.32084 valid_loss: 0.37530\n","EarlyStopping counter: 19 out of 300\n","[ 729/5000] train_loss: 0.32150 valid_loss: 0.37555\n","EarlyStopping counter: 20 out of 300\n","[ 730/5000] train_loss: 0.32214 valid_loss: 0.37529\n","EarlyStopping counter: 21 out of 300\n","[ 731/5000] train_loss: 0.32109 valid_loss: 0.37563\n","EarlyStopping counter: 22 out of 300\n","[ 732/5000] train_loss: 0.31999 valid_loss: 0.37509\n","EarlyStopping counter: 23 out of 300\n","[ 733/5000] train_loss: 0.32204 valid_loss: 0.37538\n","EarlyStopping counter: 24 out of 300\n","[ 734/5000] train_loss: 0.32110 valid_loss: 0.37477\n","EarlyStopping counter: 25 out of 300\n","[ 735/5000] train_loss: 0.32249 valid_loss: 0.37502\n","EarlyStopping counter: 26 out of 300\n","[ 736/5000] train_loss: 0.32245 valid_loss: 0.37450\n","EarlyStopping counter: 27 out of 300\n","[ 737/5000] train_loss: 0.32181 valid_loss: 0.37483\n","EarlyStopping counter: 28 out of 300\n","[ 738/5000] train_loss: 0.32041 valid_loss: 0.37587\n","EarlyStopping counter: 29 out of 300\n","[ 739/5000] train_loss: 0.32126 valid_loss: 0.37475\n","EarlyStopping counter: 30 out of 300\n","[ 740/5000] train_loss: 0.31955 valid_loss: 0.37559\n","EarlyStopping counter: 31 out of 300\n","[ 741/5000] train_loss: 0.31741 valid_loss: 0.37563\n","EarlyStopping counter: 32 out of 300\n","[ 742/5000] train_loss: 0.31848 valid_loss: 0.37608\n","EarlyStopping counter: 33 out of 300\n","[ 743/5000] train_loss: 0.32127 valid_loss: 0.37534\n","EarlyStopping counter: 34 out of 300\n","[ 744/5000] train_loss: 0.32104 valid_loss: 0.37456\n","EarlyStopping counter: 35 out of 300\n","[ 745/5000] train_loss: 0.31974 valid_loss: 0.37475\n","EarlyStopping counter: 36 out of 300\n","[ 746/5000] train_loss: 0.31976 valid_loss: 0.37450\n","EarlyStopping counter: 37 out of 300\n","[ 747/5000] train_loss: 0.31762 valid_loss: 0.37528\n","EarlyStopping counter: 38 out of 300\n","[ 748/5000] train_loss: 0.32082 valid_loss: 0.37498\n","EarlyStopping counter: 39 out of 300\n","[ 749/5000] train_loss: 0.31905 valid_loss: 0.37534\n","EarlyStopping counter: 40 out of 300\n","[ 750/5000] train_loss: 0.31754 valid_loss: 0.37509\n","EarlyStopping counter: 41 out of 300\n","[ 751/5000] train_loss: 0.32118 valid_loss: 0.37487\n","EarlyStopping counter: 42 out of 300\n","[ 752/5000] train_loss: 0.32029 valid_loss: 0.37466\n","EarlyStopping counter: 43 out of 300\n","[ 753/5000] train_loss: 0.31831 valid_loss: 0.37550\n","EarlyStopping counter: 44 out of 300\n","[ 754/5000] train_loss: 0.31865 valid_loss: 0.37635\n","EarlyStopping counter: 45 out of 300\n","[ 755/5000] train_loss: 0.31942 valid_loss: 0.37607\n","EarlyStopping counter: 46 out of 300\n","[ 756/5000] train_loss: 0.31853 valid_loss: 0.37622\n","EarlyStopping counter: 47 out of 300\n","[ 757/5000] train_loss: 0.32141 valid_loss: 0.37528\n","EarlyStopping counter: 48 out of 300\n","[ 758/5000] train_loss: 0.31842 valid_loss: 0.37586\n","EarlyStopping counter: 49 out of 300\n","[ 759/5000] train_loss: 0.31953 valid_loss: 0.37541\n","EarlyStopping counter: 50 out of 300\n","[ 760/5000] train_loss: 0.31733 valid_loss: 0.37582\n","EarlyStopping counter: 51 out of 300\n","[ 761/5000] train_loss: 0.31843 valid_loss: 0.37575\n","EarlyStopping counter: 52 out of 300\n","[ 762/5000] train_loss: 0.31890 valid_loss: 0.37645\n","EarlyStopping counter: 53 out of 300\n","[ 763/5000] train_loss: 0.31905 valid_loss: 0.37536\n","EarlyStopping counter: 54 out of 300\n","[ 764/5000] train_loss: 0.31842 valid_loss: 0.37525\n","EarlyStopping counter: 55 out of 300\n","[ 765/5000] train_loss: 0.31831 valid_loss: 0.37572\n","EarlyStopping counter: 56 out of 300\n","[ 766/5000] train_loss: 0.31751 valid_loss: 0.37476\n","EarlyStopping counter: 57 out of 300\n","[ 767/5000] train_loss: 0.31672 valid_loss: 0.37487\n","EarlyStopping counter: 58 out of 300\n","[ 768/5000] train_loss: 0.31845 valid_loss: 0.37552\n","EarlyStopping counter: 59 out of 300\n","[ 769/5000] train_loss: 0.31744 valid_loss: 0.37557\n","EarlyStopping counter: 60 out of 300\n","[ 770/5000] train_loss: 0.31614 valid_loss: 0.37646\n","EarlyStopping counter: 61 out of 300\n","[ 771/5000] train_loss: 0.31821 valid_loss: 0.37591\n","EarlyStopping counter: 62 out of 300\n","[ 772/5000] train_loss: 0.31748 valid_loss: 0.37621\n","EarlyStopping counter: 63 out of 300\n","[ 773/5000] train_loss: 0.31779 valid_loss: 0.37606\n","EarlyStopping counter: 64 out of 300\n","[ 774/5000] train_loss: 0.31761 valid_loss: 0.37609\n","EarlyStopping counter: 65 out of 300\n","[ 775/5000] train_loss: 0.31725 valid_loss: 0.37584\n","EarlyStopping counter: 66 out of 300\n","[ 776/5000] train_loss: 0.31847 valid_loss: 0.37523\n","EarlyStopping counter: 67 out of 300\n","[ 777/5000] train_loss: 0.31604 valid_loss: 0.37553\n","EarlyStopping counter: 68 out of 300\n","[ 778/5000] train_loss: 0.31906 valid_loss: 0.37549\n","EarlyStopping counter: 69 out of 300\n","[ 779/5000] train_loss: 0.31414 valid_loss: 0.37531\n","EarlyStopping counter: 70 out of 300\n","[ 780/5000] train_loss: 0.31722 valid_loss: 0.37525\n","EarlyStopping counter: 71 out of 300\n","[ 781/5000] train_loss: 0.31810 valid_loss: 0.37605\n","EarlyStopping counter: 72 out of 300\n","[ 782/5000] train_loss: 0.31830 valid_loss: 0.37572\n","EarlyStopping counter: 73 out of 300\n","[ 783/5000] train_loss: 0.31678 valid_loss: 0.37565\n","EarlyStopping counter: 74 out of 300\n","[ 784/5000] train_loss: 0.31474 valid_loss: 0.37600\n","EarlyStopping counter: 75 out of 300\n","[ 785/5000] train_loss: 0.31743 valid_loss: 0.37577\n","EarlyStopping counter: 76 out of 300\n","[ 786/5000] train_loss: 0.31616 valid_loss: 0.37559\n","EarlyStopping counter: 77 out of 300\n","[ 787/5000] train_loss: 0.31566 valid_loss: 0.37685\n","EarlyStopping counter: 78 out of 300\n","[ 788/5000] train_loss: 0.31378 valid_loss: 0.37689\n","EarlyStopping counter: 79 out of 300\n","[ 789/5000] train_loss: 0.31560 valid_loss: 0.37559\n","EarlyStopping counter: 80 out of 300\n","[ 790/5000] train_loss: 0.31826 valid_loss: 0.37526\n","EarlyStopping counter: 81 out of 300\n","[ 791/5000] train_loss: 0.31454 valid_loss: 0.37642\n","EarlyStopping counter: 82 out of 300\n","[ 792/5000] train_loss: 0.31438 valid_loss: 0.37631\n","EarlyStopping counter: 83 out of 300\n","[ 793/5000] train_loss: 0.31491 valid_loss: 0.37607\n","EarlyStopping counter: 84 out of 300\n","[ 794/5000] train_loss: 0.31620 valid_loss: 0.37707\n","EarlyStopping counter: 85 out of 300\n","[ 795/5000] train_loss: 0.31437 valid_loss: 0.37708\n","EarlyStopping counter: 86 out of 300\n","[ 796/5000] train_loss: 0.31543 valid_loss: 0.37584\n","EarlyStopping counter: 87 out of 300\n","[ 797/5000] train_loss: 0.31599 valid_loss: 0.37613\n","EarlyStopping counter: 88 out of 300\n","[ 798/5000] train_loss: 0.31759 valid_loss: 0.37478\n","EarlyStopping counter: 89 out of 300\n","[ 799/5000] train_loss: 0.31533 valid_loss: 0.37707\n","EarlyStopping counter: 90 out of 300\n","[ 800/5000] train_loss: 0.31646 valid_loss: 0.37586\n","EarlyStopping counter: 91 out of 300\n","[ 801/5000] train_loss: 0.31262 valid_loss: 0.37640\n","EarlyStopping counter: 92 out of 300\n","[ 802/5000] train_loss: 0.31334 valid_loss: 0.37686\n","EarlyStopping counter: 93 out of 300\n","[ 803/5000] train_loss: 0.31515 valid_loss: 0.37610\n","EarlyStopping counter: 94 out of 300\n","[ 804/5000] train_loss: 0.31483 valid_loss: 0.37636\n","EarlyStopping counter: 95 out of 300\n","[ 805/5000] train_loss: 0.31507 valid_loss: 0.37624\n","EarlyStopping counter: 96 out of 300\n","[ 806/5000] train_loss: 0.31525 valid_loss: 0.37570\n","EarlyStopping counter: 97 out of 300\n","[ 807/5000] train_loss: 0.31507 valid_loss: 0.37674\n","EarlyStopping counter: 98 out of 300\n","[ 808/5000] train_loss: 0.31588 valid_loss: 0.37644\n","EarlyStopping counter: 99 out of 300\n","[ 809/5000] train_loss: 0.31460 valid_loss: 0.37740\n","EarlyStopping counter: 100 out of 300\n","[ 810/5000] train_loss: 0.31510 valid_loss: 0.37642\n","EarlyStopping counter: 101 out of 300\n","[ 811/5000] train_loss: 0.31256 valid_loss: 0.37739\n","EarlyStopping counter: 102 out of 300\n","[ 812/5000] train_loss: 0.31230 valid_loss: 0.37683\n","EarlyStopping counter: 103 out of 300\n","[ 813/5000] train_loss: 0.31463 valid_loss: 0.37715\n","EarlyStopping counter: 104 out of 300\n","[ 814/5000] train_loss: 0.31446 valid_loss: 0.37714\n","EarlyStopping counter: 105 out of 300\n","[ 815/5000] train_loss: 0.31182 valid_loss: 0.37677\n","EarlyStopping counter: 106 out of 300\n","[ 816/5000] train_loss: 0.31350 valid_loss: 0.37681\n","EarlyStopping counter: 107 out of 300\n","[ 817/5000] train_loss: 0.31487 valid_loss: 0.37751\n","EarlyStopping counter: 108 out of 300\n","[ 818/5000] train_loss: 0.31354 valid_loss: 0.37698\n","EarlyStopping counter: 109 out of 300\n","[ 819/5000] train_loss: 0.31437 valid_loss: 0.37746\n","EarlyStopping counter: 110 out of 300\n","[ 820/5000] train_loss: 0.31229 valid_loss: 0.37625\n","EarlyStopping counter: 111 out of 300\n","[ 821/5000] train_loss: 0.31248 valid_loss: 0.37687\n","EarlyStopping counter: 112 out of 300\n","[ 822/5000] train_loss: 0.31346 valid_loss: 0.37603\n","EarlyStopping counter: 113 out of 300\n","[ 823/5000] train_loss: 0.31217 valid_loss: 0.37748\n","EarlyStopping counter: 114 out of 300\n","[ 824/5000] train_loss: 0.31103 valid_loss: 0.37699\n","EarlyStopping counter: 115 out of 300\n","[ 825/5000] train_loss: 0.31281 valid_loss: 0.37619\n","EarlyStopping counter: 116 out of 300\n","[ 826/5000] train_loss: 0.31290 valid_loss: 0.37659\n","EarlyStopping counter: 117 out of 300\n","[ 827/5000] train_loss: 0.31388 valid_loss: 0.37779\n","EarlyStopping counter: 118 out of 300\n","[ 828/5000] train_loss: 0.31242 valid_loss: 0.37683\n","EarlyStopping counter: 119 out of 300\n","[ 829/5000] train_loss: 0.31157 valid_loss: 0.37883\n","EarlyStopping counter: 120 out of 300\n","[ 830/5000] train_loss: 0.31361 valid_loss: 0.37709\n","EarlyStopping counter: 121 out of 300\n","[ 831/5000] train_loss: 0.31350 valid_loss: 0.37702\n","EarlyStopping counter: 122 out of 300\n","[ 832/5000] train_loss: 0.31098 valid_loss: 0.37707\n","EarlyStopping counter: 123 out of 300\n","[ 833/5000] train_loss: 0.31427 valid_loss: 0.37703\n","EarlyStopping counter: 124 out of 300\n","[ 834/5000] train_loss: 0.31049 valid_loss: 0.37764\n","EarlyStopping counter: 125 out of 300\n","[ 835/5000] train_loss: 0.31413 valid_loss: 0.37651\n","EarlyStopping counter: 126 out of 300\n","[ 836/5000] train_loss: 0.31122 valid_loss: 0.37782\n","EarlyStopping counter: 127 out of 300\n","[ 837/5000] train_loss: 0.31279 valid_loss: 0.37784\n","EarlyStopping counter: 128 out of 300\n","[ 838/5000] train_loss: 0.30970 valid_loss: 0.37798\n","EarlyStopping counter: 129 out of 300\n","[ 839/5000] train_loss: 0.31240 valid_loss: 0.37669\n","EarlyStopping counter: 130 out of 300\n","[ 840/5000] train_loss: 0.31306 valid_loss: 0.37747\n","EarlyStopping counter: 131 out of 300\n","[ 841/5000] train_loss: 0.31091 valid_loss: 0.37839\n","EarlyStopping counter: 132 out of 300\n","[ 842/5000] train_loss: 0.30956 valid_loss: 0.37772\n","EarlyStopping counter: 133 out of 300\n","[ 843/5000] train_loss: 0.31089 valid_loss: 0.37719\n","EarlyStopping counter: 134 out of 300\n","[ 844/5000] train_loss: 0.31103 valid_loss: 0.37848\n","EarlyStopping counter: 135 out of 300\n","[ 845/5000] train_loss: 0.31297 valid_loss: 0.37679\n","EarlyStopping counter: 136 out of 300\n","[ 846/5000] train_loss: 0.31142 valid_loss: 0.37781\n","EarlyStopping counter: 137 out of 300\n","[ 847/5000] train_loss: 0.30980 valid_loss: 0.37742\n","EarlyStopping counter: 138 out of 300\n","[ 848/5000] train_loss: 0.31055 valid_loss: 0.37826\n","EarlyStopping counter: 139 out of 300\n","[ 849/5000] train_loss: 0.31252 valid_loss: 0.37743\n","EarlyStopping counter: 140 out of 300\n","[ 850/5000] train_loss: 0.31273 valid_loss: 0.37760\n","EarlyStopping counter: 141 out of 300\n","[ 851/5000] train_loss: 0.30913 valid_loss: 0.37754\n","EarlyStopping counter: 142 out of 300\n","[ 852/5000] train_loss: 0.31258 valid_loss: 0.37754\n","EarlyStopping counter: 143 out of 300\n","[ 853/5000] train_loss: 0.31330 valid_loss: 0.37710\n","EarlyStopping counter: 144 out of 300\n","[ 854/5000] train_loss: 0.31016 valid_loss: 0.37828\n","EarlyStopping counter: 145 out of 300\n","[ 855/5000] train_loss: 0.31213 valid_loss: 0.37778\n","EarlyStopping counter: 146 out of 300\n","[ 856/5000] train_loss: 0.30969 valid_loss: 0.37770\n","EarlyStopping counter: 147 out of 300\n","[ 857/5000] train_loss: 0.31036 valid_loss: 0.37927\n","EarlyStopping counter: 148 out of 300\n","[ 858/5000] train_loss: 0.31105 valid_loss: 0.37693\n","EarlyStopping counter: 149 out of 300\n","[ 859/5000] train_loss: 0.30994 valid_loss: 0.37731\n","EarlyStopping counter: 150 out of 300\n","[ 860/5000] train_loss: 0.30981 valid_loss: 0.37886\n","EarlyStopping counter: 151 out of 300\n","[ 861/5000] train_loss: 0.31126 valid_loss: 0.37961\n","EarlyStopping counter: 152 out of 300\n","[ 862/5000] train_loss: 0.30939 valid_loss: 0.37832\n","EarlyStopping counter: 153 out of 300\n","[ 863/5000] train_loss: 0.31062 valid_loss: 0.37747\n","EarlyStopping counter: 154 out of 300\n","[ 864/5000] train_loss: 0.30930 valid_loss: 0.37861\n","EarlyStopping counter: 155 out of 300\n","[ 865/5000] train_loss: 0.31174 valid_loss: 0.37794\n","EarlyStopping counter: 156 out of 300\n","[ 866/5000] train_loss: 0.30997 valid_loss: 0.37820\n","EarlyStopping counter: 157 out of 300\n","[ 867/5000] train_loss: 0.30939 valid_loss: 0.38008\n","EarlyStopping counter: 158 out of 300\n","[ 868/5000] train_loss: 0.30741 valid_loss: 0.37980\n","EarlyStopping counter: 159 out of 300\n","[ 869/5000] train_loss: 0.31023 valid_loss: 0.37790\n","EarlyStopping counter: 160 out of 300\n","[ 870/5000] train_loss: 0.31182 valid_loss: 0.37773\n","EarlyStopping counter: 161 out of 300\n","[ 871/5000] train_loss: 0.30882 valid_loss: 0.37818\n","EarlyStopping counter: 162 out of 300\n","[ 872/5000] train_loss: 0.31007 valid_loss: 0.37960\n","EarlyStopping counter: 163 out of 300\n","[ 873/5000] train_loss: 0.30945 valid_loss: 0.37873\n","EarlyStopping counter: 164 out of 300\n","[ 874/5000] train_loss: 0.30797 valid_loss: 0.37920\n","EarlyStopping counter: 165 out of 300\n","[ 875/5000] train_loss: 0.30939 valid_loss: 0.37858\n","EarlyStopping counter: 166 out of 300\n","[ 876/5000] train_loss: 0.30820 valid_loss: 0.37816\n","EarlyStopping counter: 167 out of 300\n","[ 877/5000] train_loss: 0.30717 valid_loss: 0.37923\n","EarlyStopping counter: 168 out of 300\n","[ 878/5000] train_loss: 0.30886 valid_loss: 0.37823\n","EarlyStopping counter: 169 out of 300\n","[ 879/5000] train_loss: 0.30828 valid_loss: 0.37970\n","EarlyStopping counter: 170 out of 300\n","[ 880/5000] train_loss: 0.30986 valid_loss: 0.37901\n","EarlyStopping counter: 171 out of 300\n","[ 881/5000] train_loss: 0.30694 valid_loss: 0.37892\n","EarlyStopping counter: 172 out of 300\n","[ 882/5000] train_loss: 0.30977 valid_loss: 0.37898\n","EarlyStopping counter: 173 out of 300\n","[ 883/5000] train_loss: 0.30798 valid_loss: 0.37839\n","EarlyStopping counter: 174 out of 300\n","[ 884/5000] train_loss: 0.30664 valid_loss: 0.37842\n","EarlyStopping counter: 175 out of 300\n","[ 885/5000] train_loss: 0.30909 valid_loss: 0.37991\n","EarlyStopping counter: 176 out of 300\n","[ 886/5000] train_loss: 0.30782 valid_loss: 0.37887\n","EarlyStopping counter: 177 out of 300\n","[ 887/5000] train_loss: 0.30980 valid_loss: 0.37851\n","EarlyStopping counter: 178 out of 300\n","[ 888/5000] train_loss: 0.30886 valid_loss: 0.37866\n","EarlyStopping counter: 179 out of 300\n","[ 889/5000] train_loss: 0.30765 valid_loss: 0.37874\n","EarlyStopping counter: 180 out of 300\n","[ 890/5000] train_loss: 0.30945 valid_loss: 0.37964\n","EarlyStopping counter: 181 out of 300\n","[ 891/5000] train_loss: 0.30804 valid_loss: 0.38061\n","EarlyStopping counter: 182 out of 300\n","[ 892/5000] train_loss: 0.31049 valid_loss: 0.37920\n","EarlyStopping counter: 183 out of 300\n","[ 893/5000] train_loss: 0.30731 valid_loss: 0.37969\n","EarlyStopping counter: 184 out of 300\n","[ 894/5000] train_loss: 0.30566 valid_loss: 0.38047\n","EarlyStopping counter: 185 out of 300\n","[ 895/5000] train_loss: 0.30737 valid_loss: 0.38021\n","EarlyStopping counter: 186 out of 300\n","[ 896/5000] train_loss: 0.30705 valid_loss: 0.37878\n","EarlyStopping counter: 187 out of 300\n","[ 897/5000] train_loss: 0.30840 valid_loss: 0.37969\n","EarlyStopping counter: 188 out of 300\n","[ 898/5000] train_loss: 0.30563 valid_loss: 0.37854\n","EarlyStopping counter: 189 out of 300\n","[ 899/5000] train_loss: 0.30704 valid_loss: 0.37935\n","EarlyStopping counter: 190 out of 300\n","[ 900/5000] train_loss: 0.30649 valid_loss: 0.38041\n","EarlyStopping counter: 191 out of 300\n","[ 901/5000] train_loss: 0.31036 valid_loss: 0.37870\n","EarlyStopping counter: 192 out of 300\n","[ 902/5000] train_loss: 0.30714 valid_loss: 0.37990\n","EarlyStopping counter: 193 out of 300\n","[ 903/5000] train_loss: 0.30477 valid_loss: 0.38170\n","EarlyStopping counter: 194 out of 300\n","[ 904/5000] train_loss: 0.30657 valid_loss: 0.38035\n","EarlyStopping counter: 195 out of 300\n","[ 905/5000] train_loss: 0.30673 valid_loss: 0.37993\n","EarlyStopping counter: 196 out of 300\n","[ 906/5000] train_loss: 0.30921 valid_loss: 0.37858\n","EarlyStopping counter: 197 out of 300\n","[ 907/5000] train_loss: 0.30674 valid_loss: 0.37854\n","EarlyStopping counter: 198 out of 300\n","[ 908/5000] train_loss: 0.30622 valid_loss: 0.38084\n","EarlyStopping counter: 199 out of 300\n","[ 909/5000] train_loss: 0.30514 valid_loss: 0.38058\n","EarlyStopping counter: 200 out of 300\n","[ 910/5000] train_loss: 0.30669 valid_loss: 0.37948\n","EarlyStopping counter: 201 out of 300\n","[ 911/5000] train_loss: 0.30537 valid_loss: 0.38081\n","EarlyStopping counter: 202 out of 300\n","[ 912/5000] train_loss: 0.30504 valid_loss: 0.37879\n","EarlyStopping counter: 203 out of 300\n","[ 913/5000] train_loss: 0.30736 valid_loss: 0.38077\n","EarlyStopping counter: 204 out of 300\n","[ 914/5000] train_loss: 0.30656 valid_loss: 0.38106\n","EarlyStopping counter: 205 out of 300\n","[ 915/5000] train_loss: 0.30546 valid_loss: 0.38026\n","EarlyStopping counter: 206 out of 300\n","[ 916/5000] train_loss: 0.30606 valid_loss: 0.38030\n","EarlyStopping counter: 207 out of 300\n","[ 917/5000] train_loss: 0.30694 valid_loss: 0.37992\n","EarlyStopping counter: 208 out of 300\n","[ 918/5000] train_loss: 0.30530 valid_loss: 0.37984\n","EarlyStopping counter: 209 out of 300\n","[ 919/5000] train_loss: 0.30501 valid_loss: 0.38051\n","EarlyStopping counter: 210 out of 300\n","[ 920/5000] train_loss: 0.30478 valid_loss: 0.38177\n","EarlyStopping counter: 211 out of 300\n","[ 921/5000] train_loss: 0.30605 valid_loss: 0.38002\n","EarlyStopping counter: 212 out of 300\n","[ 922/5000] train_loss: 0.30242 valid_loss: 0.38165\n","EarlyStopping counter: 213 out of 300\n","[ 923/5000] train_loss: 0.30428 valid_loss: 0.38189\n","EarlyStopping counter: 214 out of 300\n","[ 924/5000] train_loss: 0.30588 valid_loss: 0.37991\n","EarlyStopping counter: 215 out of 300\n","[ 925/5000] train_loss: 0.30449 valid_loss: 0.38069\n","EarlyStopping counter: 216 out of 300\n","[ 926/5000] train_loss: 0.30795 valid_loss: 0.38073\n","EarlyStopping counter: 217 out of 300\n","[ 927/5000] train_loss: 0.30285 valid_loss: 0.38148\n","EarlyStopping counter: 218 out of 300\n","[ 928/5000] train_loss: 0.30390 valid_loss: 0.38127\n","EarlyStopping counter: 219 out of 300\n","[ 929/5000] train_loss: 0.30664 valid_loss: 0.38138\n","EarlyStopping counter: 220 out of 300\n","[ 930/5000] train_loss: 0.30305 valid_loss: 0.38154\n","EarlyStopping counter: 221 out of 300\n","[ 931/5000] train_loss: 0.30399 valid_loss: 0.38134\n","EarlyStopping counter: 222 out of 300\n","[ 932/5000] train_loss: 0.30516 valid_loss: 0.37981\n","EarlyStopping counter: 223 out of 300\n","[ 933/5000] train_loss: 0.30243 valid_loss: 0.38201\n","EarlyStopping counter: 224 out of 300\n","[ 934/5000] train_loss: 0.30402 valid_loss: 0.38106\n","EarlyStopping counter: 225 out of 300\n","[ 935/5000] train_loss: 0.30502 valid_loss: 0.38142\n","EarlyStopping counter: 226 out of 300\n","[ 936/5000] train_loss: 0.30278 valid_loss: 0.38243\n","EarlyStopping counter: 227 out of 300\n","[ 937/5000] train_loss: 0.30533 valid_loss: 0.38060\n","EarlyStopping counter: 228 out of 300\n","[ 938/5000] train_loss: 0.30301 valid_loss: 0.38238\n","EarlyStopping counter: 229 out of 300\n","[ 939/5000] train_loss: 0.30506 valid_loss: 0.38001\n","EarlyStopping counter: 230 out of 300\n","[ 940/5000] train_loss: 0.30697 valid_loss: 0.38023\n","EarlyStopping counter: 231 out of 300\n","[ 941/5000] train_loss: 0.30317 valid_loss: 0.38112\n","EarlyStopping counter: 232 out of 300\n","[ 942/5000] train_loss: 0.30366 valid_loss: 0.38099\n","EarlyStopping counter: 233 out of 300\n","[ 943/5000] train_loss: 0.30273 valid_loss: 0.38264\n","EarlyStopping counter: 234 out of 300\n","[ 944/5000] train_loss: 0.30445 valid_loss: 0.38101\n","EarlyStopping counter: 235 out of 300\n","[ 945/5000] train_loss: 0.30272 valid_loss: 0.38054\n","EarlyStopping counter: 236 out of 300\n","[ 946/5000] train_loss: 0.30291 valid_loss: 0.38119\n","EarlyStopping counter: 237 out of 300\n","[ 947/5000] train_loss: 0.30558 valid_loss: 0.37985\n","EarlyStopping counter: 238 out of 300\n","[ 948/5000] train_loss: 0.30311 valid_loss: 0.38036\n","EarlyStopping counter: 239 out of 300\n","[ 949/5000] train_loss: 0.30342 valid_loss: 0.38126\n","EarlyStopping counter: 240 out of 300\n","[ 950/5000] train_loss: 0.30419 valid_loss: 0.38105\n","EarlyStopping counter: 241 out of 300\n","[ 951/5000] train_loss: 0.30431 valid_loss: 0.38125\n","EarlyStopping counter: 242 out of 300\n","[ 952/5000] train_loss: 0.30191 valid_loss: 0.38164\n","EarlyStopping counter: 243 out of 300\n","[ 953/5000] train_loss: 0.30187 valid_loss: 0.38187\n","EarlyStopping counter: 244 out of 300\n","[ 954/5000] train_loss: 0.30264 valid_loss: 0.38222\n","EarlyStopping counter: 245 out of 300\n","[ 955/5000] train_loss: 0.30465 valid_loss: 0.38042\n","EarlyStopping counter: 246 out of 300\n","[ 956/5000] train_loss: 0.30052 valid_loss: 0.38320\n","EarlyStopping counter: 247 out of 300\n","[ 957/5000] train_loss: 0.30400 valid_loss: 0.38169\n","EarlyStopping counter: 248 out of 300\n","[ 958/5000] train_loss: 0.30097 valid_loss: 0.38339\n","EarlyStopping counter: 249 out of 300\n","[ 959/5000] train_loss: 0.30309 valid_loss: 0.38264\n","EarlyStopping counter: 250 out of 300\n","[ 960/5000] train_loss: 0.30082 valid_loss: 0.38224\n","EarlyStopping counter: 251 out of 300\n","[ 961/5000] train_loss: 0.30370 valid_loss: 0.38310\n","EarlyStopping counter: 252 out of 300\n","[ 962/5000] train_loss: 0.30095 valid_loss: 0.38385\n","EarlyStopping counter: 253 out of 300\n","[ 963/5000] train_loss: 0.30269 valid_loss: 0.38128\n","EarlyStopping counter: 254 out of 300\n","[ 964/5000] train_loss: 0.30029 valid_loss: 0.38346\n","EarlyStopping counter: 255 out of 300\n","[ 965/5000] train_loss: 0.30284 valid_loss: 0.38348\n","EarlyStopping counter: 256 out of 300\n","[ 966/5000] train_loss: 0.30054 valid_loss: 0.38347\n","EarlyStopping counter: 257 out of 300\n","[ 967/5000] train_loss: 0.30241 valid_loss: 0.38225\n","EarlyStopping counter: 258 out of 300\n","[ 968/5000] train_loss: 0.30218 valid_loss: 0.38218\n","EarlyStopping counter: 259 out of 300\n","[ 969/5000] train_loss: 0.30120 valid_loss: 0.38241\n","EarlyStopping counter: 260 out of 300\n","[ 970/5000] train_loss: 0.29894 valid_loss: 0.38268\n","EarlyStopping counter: 261 out of 300\n","[ 971/5000] train_loss: 0.30070 valid_loss: 0.38320\n","EarlyStopping counter: 262 out of 300\n","[ 972/5000] train_loss: 0.30114 valid_loss: 0.38287\n","EarlyStopping counter: 263 out of 300\n","[ 973/5000] train_loss: 0.30060 valid_loss: 0.38454\n","EarlyStopping counter: 264 out of 300\n","[ 974/5000] train_loss: 0.30189 valid_loss: 0.38487\n","EarlyStopping counter: 265 out of 300\n","[ 975/5000] train_loss: 0.29978 valid_loss: 0.38337\n","EarlyStopping counter: 266 out of 300\n","[ 976/5000] train_loss: 0.30057 valid_loss: 0.38286\n","EarlyStopping counter: 267 out of 300\n","[ 977/5000] train_loss: 0.30184 valid_loss: 0.38352\n","EarlyStopping counter: 268 out of 300\n","[ 978/5000] train_loss: 0.30235 valid_loss: 0.38294\n","EarlyStopping counter: 269 out of 300\n","[ 979/5000] train_loss: 0.30329 valid_loss: 0.38354\n","EarlyStopping counter: 270 out of 300\n","[ 980/5000] train_loss: 0.30180 valid_loss: 0.38259\n","EarlyStopping counter: 271 out of 300\n","[ 981/5000] train_loss: 0.30105 valid_loss: 0.38258\n","EarlyStopping counter: 272 out of 300\n","[ 982/5000] train_loss: 0.30010 valid_loss: 0.38247\n","EarlyStopping counter: 273 out of 300\n","[ 983/5000] train_loss: 0.30032 valid_loss: 0.38284\n","EarlyStopping counter: 274 out of 300\n","[ 984/5000] train_loss: 0.29876 valid_loss: 0.38389\n","EarlyStopping counter: 275 out of 300\n","[ 985/5000] train_loss: 0.30044 valid_loss: 0.38331\n","EarlyStopping counter: 276 out of 300\n","[ 986/5000] train_loss: 0.30081 valid_loss: 0.38345\n","EarlyStopping counter: 277 out of 300\n","[ 987/5000] train_loss: 0.29917 valid_loss: 0.38371\n","EarlyStopping counter: 278 out of 300\n","[ 988/5000] train_loss: 0.29832 valid_loss: 0.38443\n","EarlyStopping counter: 279 out of 300\n","[ 989/5000] train_loss: 0.30041 valid_loss: 0.38232\n","EarlyStopping counter: 280 out of 300\n","[ 990/5000] train_loss: 0.30222 valid_loss: 0.38441\n","EarlyStopping counter: 281 out of 300\n","[ 991/5000] train_loss: 0.29941 valid_loss: 0.38368\n","EarlyStopping counter: 282 out of 300\n","[ 992/5000] train_loss: 0.30011 valid_loss: 0.38331\n","EarlyStopping counter: 283 out of 300\n","[ 993/5000] train_loss: 0.29962 valid_loss: 0.38570\n","EarlyStopping counter: 284 out of 300\n","[ 994/5000] train_loss: 0.29978 valid_loss: 0.38443\n","EarlyStopping counter: 285 out of 300\n","[ 995/5000] train_loss: 0.30089 valid_loss: 0.38369\n","EarlyStopping counter: 286 out of 300\n","[ 996/5000] train_loss: 0.30024 valid_loss: 0.38702\n","EarlyStopping counter: 287 out of 300\n","[ 997/5000] train_loss: 0.29960 valid_loss: 0.38477\n","EarlyStopping counter: 288 out of 300\n","[ 998/5000] train_loss: 0.29898 valid_loss: 0.38358\n","EarlyStopping counter: 289 out of 300\n","[ 999/5000] train_loss: 0.30148 valid_loss: 0.38305\n","EarlyStopping counter: 290 out of 300\n","[1000/5000] train_loss: 0.30049 valid_loss: 0.38360\n","EarlyStopping counter: 291 out of 300\n","[1001/5000] train_loss: 0.29980 valid_loss: 0.38489\n","EarlyStopping counter: 292 out of 300\n","[1002/5000] train_loss: 0.29936 valid_loss: 0.38431\n","EarlyStopping counter: 293 out of 300\n","[1003/5000] train_loss: 0.29734 valid_loss: 0.38348\n","EarlyStopping counter: 294 out of 300\n","[1004/5000] train_loss: 0.29824 valid_loss: 0.38701\n","EarlyStopping counter: 295 out of 300\n","[1005/5000] train_loss: 0.29838 valid_loss: 0.38529\n","EarlyStopping counter: 296 out of 300\n","[1006/5000] train_loss: 0.29663 valid_loss: 0.38344\n","EarlyStopping counter: 297 out of 300\n","[1007/5000] train_loss: 0.29704 valid_loss: 0.38550\n","EarlyStopping counter: 298 out of 300\n","[1008/5000] train_loss: 0.29788 valid_loss: 0.38458\n","EarlyStopping counter: 299 out of 300\n","[1009/5000] train_loss: 0.29911 valid_loss: 0.38391\n","EarlyStopping counter: 300 out of 300\n","Early stopping\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tgMOjjcgmdDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["patience = 300, all epoch = 5000, final epoch=1009"],"metadata":{"id":"Yh1mlIpmyq1W"}},{"cell_type":"code","source":["device='cuda' if torch.cuda.is_available() else 'cpu'\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001,momentum=0.85)\n","new_savemodel_patience_300 = bilstm_attention(input_size=batch_data.size()[2], hidden_units=512,num_classes=1)\n","checkpoint_path='./pytorch/model/epoch5000_pa300_all1009/save_checkpoint.pt'\n","checkpoint = torch.load(checkpoint_path)\n","new_savemodel_patience_300.load_state_dict(checkpoint['model_state_dict'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czlEoJcE0lA4","executionInfo":{"status":"ok","timestamp":1683187541751,"user_tz":240,"elapsed":1441,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"f6a7357b-5072-4090-997e-c674d91df00b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["with open('./pytorch/model/epoch5000_pa300_all1009/avg_valid_losses.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    avg_valid_losses=pickle.load(f)\n","\n","with open('./pytorch/model/epoch5000_pa300_all1009/avg_train_losses.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    avg_train_losses=pickle.load(f)\n","\n","loss_df=pd.DataFrame({'train_loss':avg_train_losses, 'val_loss':avg_valid_losses})\n","loss_df.plot()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"KWgt2TJB05om","executionInfo":{"status":"ok","timestamp":1683189364559,"user_tz":240,"elapsed":2084,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"24c803f8-cc56-406f-88c4-c17d1d9168c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: >"]},"metadata":{},"execution_count":52},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["with open('./pytorch/model/epoch5000_pa300_all1009/avg_valid_losses.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    avg_valid_losses=pickle.load(f)\n","\n","with open('./pytorch/model/epoch5000_pa300_all1009/avg_train_losses.pickle', 'rb') as f:\n","    # serialize the list and write it to the file\n","    avg_train_losses=pickle.load(f)\n","\n","loss_df=pd.DataFrame({'train_loss':avg_train_losses, 'val_loss':avg_valid_losses})\n","loss_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"r6VFR-3hd4YE","executionInfo":{"status":"ok","timestamp":1686286091865,"user_tz":240,"elapsed":149,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"d1bb98c1-8b69-49d7-f498-cbf935ebc392"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          train_loss  \\\n","0  [0.6733742834447505, 0.6538803724797217, 0.648...   \n","\n","                                            val_loss  \n","0  [0.6594333929511217, 0.650376093502228, 0.6480...  "],"text/html":["\n","  <div id=\"df-b63afc91-0ff3-49fa-9e9a-a29410931adb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>val_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[0.6733742834447505, 0.6538803724797217, 0.648...</td>\n","      <td>[0.6594333929511217, 0.650376093502228, 0.6480...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b63afc91-0ff3-49fa-9e9a-a29410931adb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b63afc91-0ff3-49fa-9e9a-a29410931adb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b63afc91-0ff3-49fa-9e9a-a29410931adb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["\n","def predict(data_loader, model_1):\n","\n","    output = torch.tensor([])\n","    model_1.eval()\n","    with torch.no_grad():\n","        for X, _ in data_loader:\n","            #X=X.to(device)  \n","            y_star = model_1(X).squeeze()\n","            output = torch.cat((output, y_star), 0)\n","    \n","    return output\n","output_val=predict(val_dataloader, new_savemodel_patience_300).numpy()\n","output_test=predict(test_dataloader, new_savemodel_patience_300).numpy()\n","\n","print(len(output_val))\n","print(len(output_test))   \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMzd7YZvik-i","executionInfo":{"status":"ok","timestamp":1686253703451,"user_tz":240,"elapsed":15323,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"7a79304e-3564-4946-d42e-3fd813900381"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-405d3086c905>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(self.data[idx])\n"]},{"output_type":"stream","name":"stdout","text":["935\n","546\n"]}]},{"cell_type":"markdown","source":["extract original test labels and val labels from dataloaders since we've changed the order of the original labels when creating data_loader"],"metadata":{"id":"sKISibKhkSRv"}},{"cell_type":"code","source":["test_labels = []\n","for batch_data, batch_labels in test_dataloader:\n","    test_labels.extend(batch_labels.tolist())\n","\n","val_labels = []\n","for batch_data, batch_labels in val_dataloader:\n","    val_labels.extend(batch_labels.tolist())\n","\n","\n","print(len(test_labels))\n","print(len(val_labels))\n","\n","\n"],"metadata":{"id":"bQkQrZGR_WVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686253908388,"user_tz":240,"elapsed":405,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"97bca5da-1a11-4425-9780-399c02791929"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-405d3086c905>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(self.data[idx])\n"]},{"output_type":"stream","name":"stdout","text":["546\n","935\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import f1_score  \n","from sklearn.metrics import classification_report\n","from sklearn.metrics import cohen_kappa_score\n","\n","\n","precision, recall, thresholds = precision_recall_curve(val_labels, output_val)\n","\n","df_recall_precision = pd.DataFrame({'Precision':precision[:-1],'Recall':recall[:-1],'Threshold':thresholds})\n","df_recall_precision\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"XvfsKR9Yj6Ei","executionInfo":{"status":"ok","timestamp":1686254113216,"user_tz":240,"elapsed":242,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"b78459ba-1355-4389-f8b1-60d631bf6df5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Precision    Recall  Threshold\n","0     0.349733  1.000000  -9.574838\n","1     0.350107  1.000000  -9.180412\n","2     0.350482  1.000000  -8.965824\n","3     0.350858  1.000000  -8.865181\n","4     0.351235  1.000000  -8.854237\n","..         ...       ...        ...\n","899   1.000000  0.015291   4.837714\n","900   1.000000  0.012232   4.922742\n","901   1.000000  0.009174   5.037555\n","902   1.000000  0.006116   5.521238\n","903   1.000000  0.003058   5.648520\n","\n","[904 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-cf3039c9-192c-4c95-b7c1-a950653e0931\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Threshold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.349733</td>\n","      <td>1.000000</td>\n","      <td>-9.574838</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.350107</td>\n","      <td>1.000000</td>\n","      <td>-9.180412</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.350482</td>\n","      <td>1.000000</td>\n","      <td>-8.965824</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.350858</td>\n","      <td>1.000000</td>\n","      <td>-8.865181</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.351235</td>\n","      <td>1.000000</td>\n","      <td>-8.854237</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>899</th>\n","      <td>1.000000</td>\n","      <td>0.015291</td>\n","      <td>4.837714</td>\n","    </tr>\n","    <tr>\n","      <th>900</th>\n","      <td>1.000000</td>\n","      <td>0.012232</td>\n","      <td>4.922742</td>\n","    </tr>\n","    <tr>\n","      <th>901</th>\n","      <td>1.000000</td>\n","      <td>0.009174</td>\n","      <td>5.037555</td>\n","    </tr>\n","    <tr>\n","      <th>902</th>\n","      <td>1.000000</td>\n","      <td>0.006116</td>\n","      <td>5.521238</td>\n","    </tr>\n","    <tr>\n","      <th>903</th>\n","      <td>1.000000</td>\n","      <td>0.003058</td>\n","      <td>5.648520</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>904 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf3039c9-192c-4c95-b7c1-a950653e0931')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf3039c9-192c-4c95-b7c1-a950653e0931 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf3039c9-192c-4c95-b7c1-a950653e0931');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["np.seterr(divide='ignore', invalid='ignore')\n","f1_score = (2 * precision * recall) / (precision + recall)\n","\n","# Find the optimal threshold\n","#findex = np.argmax(f1_score)\n","findex=list(f1_score).index(max(f1_score))\n","thresholdOpt = round(thresholds[findex], ndigits = 4)\n","fscoreOpt = round(f1_score[findex], ndigits = 4)\n","recallOpt = round(recall[findex], ndigits = 4)\n","precisionOpt = round(precision[findex], ndigits = 4)\n","print('Best Threshold: {} , F-Score: {}'.format(thresholdOpt, fscoreOpt))\n","print('Recall: {}, Precision: {}'.format(recallOpt, precisionOpt))\n","\n","from sklearn.metrics import precision_score, classification_report, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n","y_pre_test=output_test.flatten().tolist()\n","defaulter_decision_test = (y_pre_test >= thresholdOpt)\n","\n","print(classification_report(test_labels, defaulter_decision_test))\n","tn, fp, fn, tp = confusion_matrix(test_labels, defaulter_decision_test).ravel()\n","print(tn, fp, fn, tp)  #recall=TP/TP+FN\n","f1_test = f1_score(test_labels, defaulter_decision_test, average='macro') # use 'micro' or 'weighted' for multi-class problems depending on the problem\n","print(f'test dataset F1 Score: {f1_test}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlbD02SplYaD","executionInfo":{"status":"ok","timestamp":1686254291251,"user_tz":240,"elapsed":109,"user":{"displayName":"Liguo Chen","userId":"01639175372495930553"}},"outputId":"308b44f2-a9df-4cd7-c6ed-56ca795d3a6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Threshold: -0.765500009059906 , F-Score: 0.7869\n","Recall: 0.8807, Precision: 0.7111\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.80      0.85       358\n","           1       0.69      0.85      0.76       188\n","\n","    accuracy                           0.82       546\n","   macro avg       0.80      0.82      0.80       546\n","weighted avg       0.83      0.82      0.82       546\n","\n","286 72 29 159\n","test dataset F1 Score: 0.8044377932316029\n"]}]}]}